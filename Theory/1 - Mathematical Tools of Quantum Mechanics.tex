\section{Mathematical Tools of Quantum Mechanics}

\subsection{Hilbert Spaces}

\textbf{Hilbert spaces}, also known as \textbf{complex vector spaces}, are defined in \textbf{Definition \ref{hilbert_spaces}}.

\begin{definition} \label{hilbert_spaces}
    A Hilbert space $\Hbt$ consists of a set of vectors $\psi$, $\psi$, $\chi$, ... and a set of scalars $a$, $b$, $c$, ... which satisfy the following four properties:

    \begin{enumerate}
        \item \textbf{$\Hbt$ is a linear space}\footnote{See \textbf{Definition \ref{linear_vector_space}} for the definition of linear vector space.}.
        \item \textbf{$\Hbt$ has a defined scalar product that is strictly positive}. The scalar product of an element $\psi$ with another element $\phi$ is in general a complex number, denoted by $(\psi,\ \phi)$. The scalar product satisfies the following properties\footnote{\textbf{Note:} Watch out for the order! Since the scalar product is a complex number, the quantity $(\psi,\ \phi) = \psi^*\phi$ is generall not equal to $(\phi,\ \psi) = \phi^*\psi$.}:
        \begin{equation}
            (\psi,\ \phi) = (\phi,\ \psi)^*
        \end{equation}
        \begin{equation}
            (\phi,\ a\psi_1 + b\psi_2) = a(\phi,\ \psi_1) + b(\phi,\ \psi_2)
        \end{equation}
        \begin{equation}
            (a\phi_1 + b\phi_2,\ \psi) = a^*(\phi_1,\ \psi) + b^*(\phi_2,\ \psi)
        \end{equation}
        \begin{equation}
            (\psi,\ \psi) = ||\psi||^2 \geq 0 \text{ (the equality holds only for } \psi = 0\text{)}
        \end{equation}
        \item \textbf{$\Hbt$ is separable}.
        \item \textbf{$\Hbt$ is complete}.
    \end{enumerate}
\end{definition}

We should note that in a scalar product $(\phi, \ \psi)$, the second factor, $\psi$, belongs to the Hilbert space $\Hbt$, while the first factor, $\phi$, belongs to its dual Hilbert space $\Hbt^*$\footnote{More on the dual space in the next section.}. The distinction between $\Hbt$ and $\Hbt^*$ is due to the fact that, as mentioned above, the scalar product is not commutative: $(\psi, \ \phi) \neq (\phi, \ \psi)$; the order matters!

\subsection{The dual space} \label{dualspace}

Given any Hilbert space $\Hbt$, one can construct another complex vector space $\Hbt^*$, called the \textbf{dual vector space}. It contains all the linear functionals in $\Hbt$, which are a special kind of operator that maps all elements of $\Hbt$ onto complex numbers\footnote{See Appendix \textbf{Section \ref{linfunct}} for more on linear functionals.}. In general, for an abstract vector space $\Hbt$:

\begin{definition}
    Given a Hilbert space $\Hbt$, the dual space $\Hbt^*$ is the vector space of all linear functionals in $\Hbt$.
\end{definition}

Therefore, all linear functionals $L: \Hbt\to \C$ live in $\Hbt^*$ ($L\in V^*$).

The reason that the dual space is so interesting for quantum mechanics is that our goal as quantum physicists is to build a mathematical model for the real world, and in the end we want to be able to extract useful values and predictions from this model. For example, we may want to know the probability of getting a certain energy; or the average position we expect in a certain state. All these are scalar values, that we need to extract from a quantum state $\ket{\psi}$, so we know we will need a linear functional someplace or other!

This may all sound really abstract at first glance, but hopefully it will become a lot clearer in the next section when we look at the \textbf{Dirac notation}.

\subsection{Dirac Notation}

In quantum mechanics, we use the Dirac notation to represent wave functions:

\begin{itemize}
    \item We call the elements of $\Hbt$ ``ket'' vectors, and we represent them as $\ket{\psi}\in\Hbt$.
    \item We call the elements of $\Hbt^*$ ``bra'' vectors, and we represent them as $\bra{\phi}\in\Hbt^*$.
\end{itemize}

Bra vectors are operators that linearly map elements of $\Hbt$ into complex numbers:

\begin{equation}
    \begin{matrix}
    \bra{\phi}: & \Hbt & \rightarrow & \C\\
    \bra{\phi}: & \ket{\psi} & \rightarrow & \braket{\phi}{\psi}\\
    \end{matrix}
\end{equation}

\subsubsection{Inner product and bra-ket notation}

Notice that, when we put a bra and a ket together ($\bra{\phi}\ket{\psi}$), they look suspiciously like an inner product in this notation: $\braket{\phi}{\psi}$. If we go back at how our $L_x$ operator in $\R^2$ acts on a column vector:

\begin{equation}
    L_x\begin{bmatrix}
        a \\ b
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 \\
    \end{bmatrix}\cdot
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = 1 \cdot a + 0\cdot b = a
\end{equation}

Notice that its action is the same as if we were taking the dot product with the $x$ unit vector:

\begin{equation}
    x\cdot\begin{bmatrix}
        a \\ b
    \end{bmatrix} = 
    \begin{bmatrix}
        1 \\ 0
    \end{bmatrix}\cdot
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = 1 \cdot a + 0\cdot b = a
\end{equation}

In fact, when a linear functional in $\R^n$ acts on any vector, it can be written equivalently as a dot product with the corresponding column vector:

\begin{equation}
    L_x \vec{v} = L_x^T\cdot \vec{v}
\end{equation}

This is actually a very general mathematical fact, rooted within something called the \textbf{Riesz Representation Theorem}:

\begin{theorem}
    \textbf{(Riesz Representation Theorem)} For any linear functional $L_\phi$, the action of $L_\phi$ is equivalent to taking the inner product with some unique vector $\vec{\phi}$.
\end{theorem}

In our example of $L_x$, we have that $\vec{\phi} = \vec{x} = [1\ 0]^T$:

\begin{equation}
    L_x\vec{v} = \begin{bmatrix}
        1 \\ 0
    \end{bmatrix}\cdot \vec{v}    
\end{equation}

This is the reason for the suggestive notation for bra vectors: they are operators whose action on a ket is mathematically equivalent to taking the inner product with said ket:
\begin{equation}
    \bra{\phi}\ket{\psi} = \braket{\phi}{\psi}
\end{equation}

That is the power of bra-ket notation: it has the Riesz Representation Theorem baked right into it. Whatever you do, breaking apart inner products and putting together bras and kets, you will always have something that makes mathematical sense. Although bra and the inner product are two entities that are completely different mathematically, the bra-ket notation makes their connection completely seamless, thanks to the Riesz Representation Theorem.

% https://www.youtube.com/watch?v=lRR-qgjaKlg

\subsubsection{Properties of bras and kets}

Some properties that arise naturally from the Dirac notation:

\begin{equation}
    \braket{\psi}{\lambda_1\phi_1+\lambda_2\phi_2} = \lambda_1\braket{\psi}{\phi_1} + \lambda_2\braket{\psi}{\phi_2}
\end{equation}
\begin{equation}
    \braket{\lambda_1\psi+\lambda_2\psi_2}{\phi} = \lambda_1^*\braket{\psi_1}{\phi} + \lambda_2^*\braket{\psi_2}{\phi}
\end{equation}
\begin{equation}
    \braket{\psi}{\phi} = \braket{\phi}{\psi}^*
\end{equation}
\begin{equation}
    \braket{\psi}{\psi}\text{ is real, positive and only zero if }\ket{\psi} = 0
\end{equation}


\subsection{Linear operators}

A linear map is defined as:

\begin{definition} \label{linear_map}
    A linear map (or linear operator) is a mathematical entity $A$ that associates a function with another function such that:
    \begin{equation}
        A (\lambda_1\psi_1 + \lambda_2\psi_2) = \lambda_1 A\psi_1 + \lambda_2 A\psi_2
    \end{equation}
\end{definition}


In the quantum mechanical context, we can see them as entities that transform a ket into another ket. Some example linear operators are:
\begin{itemize}
    \item \textbf{Commutator:} The commutator of two operators $A$ and $B$ is defined as:
    \begin{equation}
        [A, B] \equiv AB - BA
    \end{equation}
    Two operators are said to commute if their commutator is equal to zero, and hence $AB = BA$. See \textbf{Section \ref{uncertainty_relation}} to see an interesting application of commutator algebra for finding the uncertainty products of two operators.
    \item \textbf{Anti-commutator:} The anti-commutator of two operators $A$ and $B$ is defined as:
    \begin{equation}
        \{A, B\} \equiv AB + BA
    \end{equation}
    \item \textbf{Projector:} $P_\phi = \ket{\phi}\bra{\phi}$. The projector operator $P_\phi$ acting on a ket $\ket{\psi}$ gives a new ket that is proportional to $\ket{\phi}$. The coefficient of proportionality is the scalar product $\braket{\phi}{\psi}$.\footnote{\textit{Proof:} $P_\phi\ket{\psi} = \ket{\phi}\bra{\phi}\ket{\psi} = \ket{\phi}\braket{\phi}{\psi} = \braket{\phi}{\psi}\ket{\phi}$.}
    \item \textbf{Inverse:} assuming it exists, the inverse operator $A^{-1}$ of the operator $A$, when applied to $A$, gives the identity operator. Also, $A$ is the inverse of $A^{-1}$, so that $AA^{-1} = A^{-1}A = \Imat$.
    \item \textbf{Hermitian conjugation:} the hermitian conjugate (or adjoint) $A^\dagger$ of an operator $A$ is obtained by interchanging the columns of the operator by its rows, and taking the complex conjugate of all elements. For example:
    \begin{equation}
        A = \begin{bmatrix}
            i & 1 \\
            3-i & -i
        \end{bmatrix}\rightarrow
        A^\dagger = \begin{bmatrix}
            -i & 3 + i \\
            1 & i
        \end{bmatrix}
    \end{equation}
    Some properties of the adjoint are:
    \begin{enumerate}
        \item[a)] $\left(A^\dagger\right)^\dagger = A$.
        \item[b)] $\left(\lambda A\right)^\dagger = \lambda^*A^\dagger$.
        \item[c)] $\left(A + B\right)^\dagger = A^\dagger + B^\dagger$.
        \item[d)] $\left(A B\right)^\dagger = B^\dagger A^\dagger$.
        \item [e)] $\left(\ket{u}\bra{v}\right)^\dagger = \ket{v}\bra{u}$.\footnote{\textit{Proof:} $\bra{\phi}\left(\ket{u}\bra{v}\right)^\dagger\ket{\psi} = \left[\bra{\psi}\left(\ket{u}\bra{v}\right)\ket{\phi}\right]^* = \braket{\psi}{u}^*\braket{v}{\phi}^* = \braket{u}{\psi}\braket{\phi}{v} = \braket{\phi}{v}\braket{u}{\psi} = \bra{\phi}\left(\ket{v}\bra{u}\right)\ket{\psi}$}
    \end{enumerate}
    The adjoint of a bra is its ket, and the adjoint of a ket is its bra.
    To obtain the hermitian conjugate of an expression:
    \begin{enumerate}
        \item[a)] Replace constants with their complex conjugate: $\lambda \to \lambda^*$.
        \item[b)] Replace operators with their Hermitian conjugates: $A\to A^\dagger$.
        \item[c)] Replace kets with bras: $\ket{\phi}\to\bra{\phi}$.
        \item[d)] Replace bras with kets: $\bra{\phi}\to\ket{\phi}$.
        \item[e)] Reverse the order of factors: $A\ket{\phi} \to \bra{\phi}A^\dagger$.

    \end{enumerate}
\end{itemize}

A special case of linear operators are \textbf{unitary operators}:

\begin{definition}
    A linear operator $U$ is said to be unitary if its inverse $U^{-1}$ is equal to its adjoint $U^\dagger$, so that $U^{-1} = U^\dagger$ and $U^\dagger U = UU^\dagger = \Imat$.
\end{definition}

Another special case are \textbf{Hermitian operators}:

\begin{definition}
    An operator $A$ is said to be Hermitian if $A^\dagger = A$.
\end{definition}

and \textbf{anti-Hermitian operators}:
\begin{definition}
    An operator $A$ is said to be anti-Hermitian if $A^\dagger = -A$.
\end{definition}

An example of a Hermitian operator is the projector operator, as $P_\phi^\dagger = \left(\ket{\phi}\bra{\phi}\right)^\dagger = \ket{\phi}\bra{\phi} = P_\phi$.

\subsubsection{Expected value of an operator}

In order to define the expected value of an operator, we first need to define the \textbf{matrix element}:

\begin{definition}
    Let $\ket{\psi}, \ \ket{\phi}$ be two kets, we call the matrix element of an operator $A$ between $\ket{\psi}$ and $\ket{\phi}$ the quantity $\bra{\psi}(A\ket{\phi})$. 
\end{definition}

Note that the matrix element of an operator $A$ between $\ket{\psi}$ and $\ket{\phi}$ is a complex number, and it is equal to the scalar product of $\ket{\psi}$ with the ket $A\ket{\phi}$. If we now define the expected value of an operator:

\begin{definition}
    The expected value $\langle A \rangle_\psi$ of $A$ in the state $\ket{\psi}$ is defined as the matrix element of $A$ between $\ket{\psi}$ and itself:
    \begin{equation}
        \langle A \rangle_\psi = \braket{\psi|A|\psi}
    \end{equation}
\end{definition}

It is easy to see that, if $\psi$ is chosen to be an eigenvector of $A$, then the expected value of $A$ in the state $\ket{\psi}$ is equal to the eigenvalue $\lambda$ of $A$ corresponding to the eigenvector $\ket{\psi}$:
\begin{equation}
    \braket{\psi|A|\psi} = \bra{\psi} (A \ket{\psi}) = \bra{\psi} (\lambda \ket{\psi}) = \lambda\braket{\psi|\psi} = \lambda
\end{equation}

This means that, for an arbitrary vector $\phi$ expressed as a linear combination of eigenvectors $\psi_i$ of $A$:
\begin{equation}
    \ket{\phi} = \sum_i c_i\ket{\psi_i}
\end{equation}
we have:
\begin{equation*}
    \braket{\phi|A|\phi} = \sum_i\sum_j c_i^*c_j\braket{\psi_i|A|\psi_j} = \sum_i\sum_j c_i^*c_j\braket{\psi_i|\lambda_j|\psi_j} = \sum_i\sum_j c_i^*c_j\lambda_j\braket{\psi_i|\psi_j} = 
\end{equation*}
\begin{equation} \label{expected_value}
    = \sum_i\sum_j c_i^*c_j\lambda_j\delta_{ij} = \sum_i c_i^*c_i\lambda_i = \sum_i |c_i|^2\lambda_i
\end{equation}

\subsection{Closure relation}

For a set of vectors to form a basis of a Hilbert space $\Hbt$, they must fulfil the \textbf{closure relation} (also known as the completeness relation). In simple terms, if the set of vectors fulfills the closure relation, it means that with those vectors you can reach all possible directions in $\Hbt$, and any $\ket{\psi}\in\Hbt$ is a linear combination of those basis vectors. In our general Hilbert space:

\begin{definition}
    A set of vectors $\{\ket{A_1}, \ket{A_2}, ...\}$ in a Hilbert space $\Hbt$ form a basis for $\Hbt$ if and only if they fulfil the closure relation:
    \begin{equation}
        \Imat = \sum_i\ket{A_i} \bra{A_i}.
    \end{equation}
\end{definition}

Proof of this relation is given in \textbf{Section \ref{closure_relation_proof}}.

\subsection{Wave function space $\F$}

The wave function in quantum mechanics is an object whose modulus squared is a probability density function. If we look back at \textbf{Definition \ref{hilbert_spaces}}, we can see that, from a physical point of view, the set $\Hbt$ is clearly too wide in scope for our purposes. We need to restrict it to a subset of $\Hbt$ that is physically meaningful. This subset is called the \textbf{wave function space} $\F$, and it retains only the functions $\psi$ of $\Hbt$ which are everywhere defined, continuous, and infinitely differentiable. In addition, the functions of $\F$ must be normalizable by an arbitrary multiplicative constant in such a way that the area under the curve $|\psi|^2$ is exactly equal to $1$.

\subsection{Basis of the wave function space}

If $\{\ket{\psi_i}\}$ is a basis for a Hilbert space $\Hbt$ (in particular, the subset $\F$), we can expand every arbitrary vector $\ket{\Psi}$ according to this basis:
\begin{equation}
    \ket{\Psi} = \sum_i c_i \ket{\psi_i}
\end{equation}
so that we have\footnote{$\delta_{ij}$ is known as Kronecker's delta, and is equal to $1$ if $i=j$ and $0$ otherwise.}:
\begin{equation}
    \braket{\psi_i}{\Psi} = \sum_j c_i \braket{\psi_i}{\Psi} = \sum_j c_i\delta_{ij} = c_i
\end{equation}
The choice of basis is arbitrary, and depending on the choice we make, we obtain different representations of state space. There are many different representations, which often have to do with physical properties of the system. 

If you are familiar with basic quantum mechanics, you will probably have seen the position representation of the wave function, $\Psi\left(\vec{r}\right)$. This representation of state space is particularly useful for working with position in a quantum system. However, it is not the \textit{only} representation of state space that can have. What does this mean? Well, here, there is an important concept to understand, which is the difference between a vector and its representation: a vector is a mathematical entity that, once defined, is the same all the time, no matter where we look at it from or which basis we express it in. Its representation, however, may differ, depending on which basis we choose to represent it in. If the basis changes, the coordinates will also change, even though the vector is still the same.

For example, a vector $\vec{v}_B = (a, b)$ expressed in the basis $B=\{(1, 0),\ (0, 1)\}$ will change to $\vec{v}_{B'} = (b/2, a)$ $B'=\{(0, 2),\ (1, 0)\}$. The coordinates of the vector have changed, but we can see that both representations refer to the same vector:

\begin{equation}
    \vec{v}_B = a\cdot \begin{pmatrix}
        1 \\ 0
    \end{pmatrix} + b \cdot \begin{pmatrix}
        0 \\ 1
    \end{pmatrix} = 
    \frac{b}{2}\cdot \begin{pmatrix}
        0 \\ 2
    \end{pmatrix} + a \cdot \begin{pmatrix}
        1 \\ 0
    \end{pmatrix} = \vec{v}_{B'}
\end{equation}

Just as in this example, the position representation of the wave function vector space is only one of its many possible representations. The functions $\psi_i\left(\vec{r}\right)$ form the basis for the position representation of state space. Other representations, like the momentum representation, can be useful in certain situations, as we will see later on. 


\subsection{Representations in state space}

When studying quantum mechanical systems, we need a way to represent quantum states. We do that by choosing an orthonormal basis, either discrete or continuous, in the state space $\F$. Vectors and operators are then represented in this basis by numbers: components for the vectors and matrix elements for the operators.

As we mentioned before, the choice of a representation is, in principle, arbitrary. In fact, it depends on the particular problem being studied: in each case, one chooses the representation that leads to the simplest calculations. 

Most useful bases come as eigenstates of some pertinent operator\footnote{More on this later{\color{red}add link}, but the idea is that, if we represent the state space in the basis of eigenstates (analogous to eigenvectors) of an operator, then that operator will be expressed in that basis as a diagonal matrix, where the elements of the diagonals are the eigenvalues of the operator. This makes calculations very easy.}. So far, we have mentioned the position and the momentum representation. These bases deal with the position and momentum operators, but you can think of many others. For example, the eigenstates of the Hamiltonian for some physical system are often used, especially when solving the Schrödinger equation. These might also be infinite dimensional but can be discrete, as opposed to the continuous bases of position and momentum.

\subsubsection{General representation}

For a general representation of state space (wave function space, $\F$), the elements of $\F$ are functions $\psi(\vec{\xi}),\ \phi(\vec{\xi})$, and the inner product in $\F$ is defined as:

\begin{equation}
    \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) \equiv \int\phi^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
\end{equation}

The length (norm) of a vector is given by:

\begin{equation}
    \text{length}\left(\psi(\vec{\xi})\right)=\sqrt{\left(\psi(\vec{\xi}), \ \psi(\vec{\xi})\right)} = \sqrt{\int\psi^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}} = \sqrt{\int\left|\psi(\vec{\xi}) \right|^2d\vec{\xi}}
\end{equation}

As the vectors of the basis $\{\phi_i\}$ of $\F$ are orthonormal, we have that:
\begin{equation}
    \left(\phi_i(\vec{\xi}), \ \phi_j(\vec{\xi})\right) = \delta_{ij}
\end{equation}

\subsubsection{Discrete orthonormal bases}

A \textbf{discrete orthonormal basis} is defined as:

\begin{definition}
    A countable set of functions $\{u_i\}$ is called orthonormal if:
    \begin{equation}
        \left(u_i(\vec{\xi}), \ u_j(\vec{\xi})\right) = \delta_{ij}
    \end{equation}
    And it constitutes a basis for $\F$ if every function in $\F$ can be written as a linear combination of the functions of the basis in exactly one way:
    \begin{equation}
        \psi(\vec{\xi}) = \sum_i c_i u_i(\vec{\xi})
    \end{equation}
    with the coefficients being:
    \begin{equation}
        c_i = \left(u_i(\vec{\xi}), \ \psi(\vec{\xi})\right) = \int u_i^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
    \end{equation}    
    Note that, in a discrete orthonormal basis, all basis vectors $u_i$ are elements belonging to $\F$. In other words, every basis vector $u_i$ is a valid physical state for the system in the space $\F$. As we will see, this will \textit{not} be the case for continuous orthonormal ``bases''.
\end{definition}

For a discrete orthonormal basis, we can express the \textbf{scalar product in terms of the components} as:
\begin{equation}
    \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) = \left(\sum_i b_iu_i(\vec{\xi}), \ \sum_j c_ju_j(\vec{\xi})\right) = \sum_{i, \ j} b^*_i c_j \left(u_i(\vec{\xi}), \ u_j(\vec{\xi})\right) = \sum_{i,\ j} b^*_i c_i\delta_{ij} = \sum_i b^*_i c_i
\end{equation}

With a similar proof as in \textbf{Section \ref{closure_relation_proof}}, we can find the closure relation for a discrete orthonormal basis:

\begin{equation*}
    \psi(\vec{\xi}) = \sum_i c_i u_i(\vec{\xi}) = \sum_i \left(u_i(\vec{\xi}), \ \psi(\vec{\xi})\right) u_i(\vec{\xi}) = \sum_i \left( \int u_i^* (\vec{\xi}') \psi(\vec{\xi}')d\vec{\xi}'\right)u_i(\vec{\xi})  =    
\end{equation*}
\begin{equation}
    = \int \left( \sum_i u_i^* (\vec{\xi}') u_i(\vec{\xi})\right)\psi(\vec{\xi}')d\vec{\xi}'
\end{equation}

Therefore, the term in the parenthesis must be equal to $1$ for $\vec{\xi} =\vec{\xi}'$ and zero for every other case, so we obtain the \textbf{closure relation for a discrete orthonormal basis}\footnote{$\delta(\vec{\xi}-\vec{\xi}')$ is known as Dirac's delta function, and it is equal to $1$ if $\vec{\xi} = \vec{\xi}'$ and $0$ otherwise. It can also be expressed as the integral $\delta (\vec{\xi} - \vec{\xi}') = \frac{1}{(2\pi)^3}\int e^{i\vec{k}\cdot (\vec{\xi}-\vec{\xi}')}d^3k$.}:
\begin{equation}
    \sum_i u^*(\vec{\xi}')u_i(\vec{\xi}) = \delta(\vec{\xi} - \vec{\xi}')
\end{equation}

\subsubsection{Continuous orthonormal bases}

A \textbf{continuous orthonormal basis} is defined as:

\begin{definition}
    A continuous set of functions $\{\omega_\alpha\}$, labelled by a continuous index $\alpha$, is called orthonormal if:
    \begin{equation}
        \left(\omega_\alpha(\vec{\xi}), \ \omega_{\alpha'}(\vec{\xi})\right) = \delta(\alpha - \alpha')
    \end{equation}
    And it constitutes a basis for $\F$ if every function in $\F$ can be written as a linear combination of the functions of the basis in exactly one way:
    \begin{equation}
        \psi(\vec{\xi}) = \int c(\alpha) \omega_\alpha(\vec{\xi})d\alpha
    \end{equation}
    with the continuous coefficient being:
    \begin{equation}
        c(\alpha) = \left(\omega_\alpha(\vec{\xi}), \ \psi(\vec{\xi})\right) = \int \omega_\alpha^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
    \end{equation}   
    Note that, $\braket{\omega_\alpha(\vec{\xi})|\omega_{\alpha'}(\vec{\xi})} = \delta(\alpha - \alpha')$ implies that the functions $\omega_\alpha$ are not normaliseable\footnote{As $\braket{\omega_\alpha(\vec{\xi})|\omega_{\alpha'}(\vec{\xi})} = \delta(\alpha - \alpha')$ would imply that $\|\omega_\alpha\|^2 = \braket{\omega_\alpha(\vec{\xi})|\omega_\alpha(\vec{\xi})} = \delta(\alpha - \alpha) = \infty \neq 1$. So, it turns out that any basis in $\F$ has to be a discrete basis with an orthogonality condition expressed in terms of a Kronecker delta instead of a Dirac delta.}, so these functions are \textit{not} vectors in $\F$. Therefore, strictly speaking, they cannot be a basis for $\F$. In other words, basis vectors $\omega_\alpha$ are \textit{not} valid physical states for the system in the space $\F$.  Rather, they are a mathematical tool that can help us to perform calculations in certain scenarios, and are formalised in what is known as a rigged Hilbert space\footnote{See \href{https://physics.stackexchange.com/a/623416/283847}{this} and \href{https://physics.stackexchange.com/a/359982/283847}{this} post for more information.}. However, we will still refer to them as ``basis vectors'' for simplicity.
\end{definition}

For a continuous orthonormal basis, we can express the \textbf{scalar product in terms of the continuous coefficients} as:
\begin{equation*}
    \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) = \left(\int b(\alpha) \omega_\alpha(\vec{\xi})d\alpha, \ \int c(\alpha') \omega_{\alpha'}(\vec{\xi})d\alpha'\right) = \sum_{i, \ j} b^*_i c_j \left(u_i(\vec{\xi}), \ u_j(\vec{\xi})\right) = 
\end{equation*}
\begin{equation*}
    = \int\left(\int b^*(\alpha)c(\alpha') (\omega_\alpha(\vec{\xi}), \omega_{\alpha'}(\vec{\xi}))d\alpha\right) d\alpha' = \int\left(\int b^*(\alpha)c(\alpha') \delta(\alpha-\alpha') d\alpha\right) d\alpha'
\end{equation*}
\begin{equation}
    = \int b^*(\alpha)c(\alpha) d\alpha
\end{equation}

With a similar proof as in \textbf{Section \ref{closure_relation_proof}}, we can find the closure relation for a continuous orthonormal basis:

\begin{equation*}
    \psi(\vec{\xi}) = \int c(\alpha) \omega_\alpha(\vec{\xi})d\alpha = \int \left(\omega_\alpha(\vec{\xi}), \ \psi(\vec{\xi})\right) \omega_\alpha(\vec{\xi})d\alpha = \int \left(\int \omega_\alpha^*(\vec{\xi}')\psi(\vec{\xi}')d\vec{\xi}'\right) \omega_\alpha(\vec{\xi})d\alpha  =    
\end{equation*}
\begin{equation}
    = \int \left(\int \omega_\alpha^*(\vec{\xi}')\omega_\alpha(\vec{\xi})d\alpha\right) \psi(\vec{\xi}')d\vec{\xi}'
\end{equation}

Therefore, the term in the parenthesis must be equal to $1$ for $\vec{\xi} =\vec{\xi}'$ and zero for every other case, so we obtain the \textbf{closure relation for a continuous orthonormal basis}:
\begin{equation}
    \int \omega_\alpha^*(\vec{\xi}')\omega_\alpha(\vec{\xi})d\alpha = \left( \omega_\alpha (\vec{\xi}'),\ \omega_\alpha (\vec{\xi}) \right) = \delta(\vec{\xi} - \vec{\xi}')
\end{equation}

\textbf{Fourier transform}

Take the inverse fourier transform of the position wave function, for example:
\begin{equation}
    \psi(\vec{r}) = \frac{1}{\sqrt{2\pi\hbar}}\int \overline\psi(\vec{p})e^{i\vec{p}\cdot \vec{r}/\hbar}d \vec{p} = \int \overline\psi(\vec{p})v_{\vec{p}}(\vec{r})d\vec{p}, \qquad v_{\vec{p}}(\vec{r}) = \frac{1}{\sqrt{2\pi\hbar}}e^{i\vec{p} \cdot\vec{r}/\hbar} \text{ (plane wave)}
\end{equation}
Notice that this is the same as:
\begin{equation}
    \psi(\vec{\xi}) = \int c(\alpha) \omega_\alpha(\vec{\xi})d\alpha
\end{equation}
where:
\begin{equation}
    \alpha \to \vec{p} \text{ (continuous index)}, \quad c(\alpha) \to \overline\psi(\vec{p}), \quad \omega_\alpha(\vec{\xi}) \to v_{\vec{p}}(\vec{r}) \text{ (basis functions)}
\end{equation}
And the ``continuous coefficient'' function $\overline\psi$ can be found as:
\begin{equation}
    \overline\psi(\vec{p}) = (v_{\vec{p}}, \psi) = \int v_{\vec{p}}^*(\vec{r})\psi(\vec{r})d\vec{r}
\end{equation}


It would seem that the set of uncountable $v_{\vec{p}}$ functions is a basis for $\F$, however, the integral of $\left|v_{\vec{p}}(\vec{r})\right|^2 = \frac{1}{2\pi\hbar}$ diverges, so $v_{\vec{p}}(\vec{r})\notin \F$.

\textbf{Delta function}

In the same way, we can introduce the set of functions $\{\xi_{\vec{r}_0}(\vec{r})\}$ of $\vec{r}$, labelled by the continuous index $\vec{r}_0$ and defined as:
\begin{equation}
    \xi_{\vec{r}_0}(\vec{r}) = \delta(\vec{r} - \vec{r}_0)
\end{equation}
$\{\xi_{\vec{r}_0}(\vec{r})\}$ represents the set of delta functions centered at each of the points $\vec{r}_0$ of space. Clearly, $\xi_{\vec{r}_0}(\vec{r})$ is not square integrable, so $\xi_{\vec{r}_0}(\vec{r})\notin \F$. Then, consider the relations:
\begin{equation} \label{delta_1}
    \psi(\vec{r}) = \int \psi(\vec{r}_0)\delta(\vec{r} - \vec{r}_0) \ d^3r_0 = \int \psi(\vec{r}_0)\xi_{\vec{r}_0}(\vec{r})\ d^3r_0
\end{equation}
\begin{equation} \label{delta_2}
    \psi(\vec{r}_0) = (\xi_{\vec{r}_0}, \psi) = \int \delta(\vec{r}_0 - \vec{r})\psi(\vec{r}) \ d^3r = \int \xi^*_{\vec{r}_0}(\vec{r})\psi(\vec{r}) \ d^3r
\end{equation}

\textbf{Equation \ref{delta_1}} expresses the fact that every function $\psi(\vec{r})\in\F$ can be expanded in terms of the $\xi_{\vec{r}_0}(\vec{r})$ functions in exactly one way. \textbf{Equation \ref{delta_2}} shows that the value of the ``continuous coefficient'' corresponding to $\xi_{\vec{r}_0}(\vec{r})$ is preciesly $\psi(\vec{r}_0)$. 

The usefulness of the continuous bases that we have just introduced is revealed more clearly in what follows. However, we must not lose sight of the following point: a physical state must always correspond to a square-integrable wave function. In no case can $v_{\vec{p}}(\vec{r})$ or $\xi_{\vec{r}_0}(\vec{r})$ represent the state of a particle. These functions are nothing more than intermediaries, very useful in calculations involving operations on the wave functions $\psi(\vec{r})$ which are used to describe a physical state.\footnote{\color{red}aclarar}



\subsection{Matrix formulation of quantum mechanics}
\subsubsection{Matrix representation in discrete bases}

Recall that, for a basis $\{\ket{\phi_n}\}$ of $\F$, we can write any $\psi\in\F$ as a linear combination of the basis vectors:

\begin{equation}
    \ket{\psi} = \sum_n c_n\ket{\phi_n}
\end{equation}

where $c_n = \braket{\phi_n|\psi}$ represents the projection of $\ket{\psi}$ onto $\ket{\phi_n}$. So, within the basis $\{\ket{\phi_n}\}$, the ket $\ket{\psi}$ is represented by the set of its components, $c_1, c_2, ...$, along $\ket{\phi_1}, \ket{\phi_2}, ...$, respectively. Hence, we can write the ket $\ket{\psi}$ as a column vector:
\begin{equation}
    \ket{\psi} \to \begin{pmatrix}
        \braket{\phi_1|\psi} \\ \braket{\phi_2|\psi} \\ \vdots \\ \braket{\phi_n|\psi}
    \end{pmatrix} = \begin{pmatrix}
        c_1 \\ c_2 \\ \vdots \\ c_n
    \end{pmatrix}
\end{equation}
The bra $\bra{\psi}$ is represented by the row vector:
\begin{equation}
    \bra{\psi} \to \begin{pmatrix}
        \braket{\psi|\phi_1} & \braket{\psi|\phi_2} & \hdots & \braket{\psi|\phi_n}
    \end{pmatrix} = \begin{pmatrix}
        \braket{\phi_1|\psi}^* & \braket{\phi_2|\psi}^* & \hdots & \braket{\phi_n|\psi}^* 
    \end{pmatrix} =
    \begin{pmatrix}
        c_1^* & c_2^* & \hdots & c_n^*
    \end{pmatrix}
\end{equation}

Just as kets and bras are respresented by column and row vectors, respectively, operators are represented by square matrices.

\subsection{Eigenvalues and eigenvectors of an operator}

We can define the \textbf{eigenvectors of an operator} as:

\begin{definition}
    A state vector $\ket{\psi}$ is said to be an eigenvector (also called eigenket or eigenvector) of an operator $A$ if it is a solution of the eigenvalue equation:
    \begin{equation}
        A\ket{\psi} = a\ket{\psi}
    \end{equation}
    where $a$ is a complex number, called an eigenvalue of $A$.
\end{definition}

Some theorems regarding eigenvectors and eigenvalues are:
\begin{theorem}
    The eigenvalues of the inverse $A^{-1}$ of an operator $A$ are the inverse (with respect to the multiplication, $1/a$) of the eigenvalues $a$ of $A$. \textit{Proof:}
    \begin{equation}
        A\ket{\psi} = a\ket{\psi}\to A^{-1}A\ket{\psi} = aA^{-1}\ket{\psi} \to \ket{\psi} = a A^{-1}\ket{\psi}\to A^{-1}\ket{\psi} = \frac1a \ket{\psi}
    \end{equation}
\end{theorem}
\begin{theorem}
    For a Hermitian operator $A$, all of its eigenvalues are real and the eigenvectors corresponding to different eigenvalues are orthogonal\footnote{Proof in \textbf{Section \ref{eigenvalues_real_orthogonal}}.}.
\end{theorem}
\begin{theorem} \label{commuting_operator_base_thm}
    If two Hermitian operators, $A$ and $B$, commute and if $A$ has no degenerate eigenvalue, then each eigenvector of $A$ is also an eigenvector of $B$. In addition, we can construct a common orthonormal basis that is made of the joint eigenvectors of $A$ and $B$\footnote{Proof in \textbf{Section \ref{commuting_operator_basis}}.}.
\end{theorem}


\subsection{Position representation of state space}

In the position representation, the basis consists of an infinite set of vectors $\{\ket{\vec{r}}\}$, which are eigenkets of the position operator $\vec{R}$:
\begin{equation}
    \vec{R}\ket{\vec{r}} = \vec{r}\ket{\vec{r}}
\end{equation}
where $\vec{r}$, the position vector, is the eigenvalue of the position operator $\vec{R}$. The orthonormality and completeness relations are given by\footnote{Remember that elements of an uncountable set of functions such as this \textit{cannot} be elements of $\F$.}:
\begin{equation}
    \braket{\vec{r}|\vec{r}\ '} = \delta(\vec{r} - \vec{r}\ ')
\end{equation} 
\begin{equation}
    \int \ket{\vec{r}}\bra{\vec{r}} d^3r = \Imat
\end{equation}

And every state vector $\ket{\psi}$ can be expanded in terms of the position eigenkets $\ket{\vec{r}}$ as:
\begin{equation}
    \ket{\psi} = \int \braket{\vec{r}|\psi} \ket{\vec{r}} d^3 r = \int \psi(\vec{r}) \ket{\vec{r}} d^3 r
\end{equation}

where the \textbf{wave function} $\psi(\vec{r}) = \braket{\vec{r}|\psi}$ denotes the components of $\psi$ in the $\{\ket{\vec{r}}\}$ basis. The quantity $|\braket{\vec{r}|\psi}|^2d^3 r = |\psi(\vec{r})|^2d^3r$ represents the probability of finding the system inside the volume element $d^3r$.

The scalar product between two state vectors $\ket{\psi}$ and $\ket{\phi}$ can be written as:
\begin{equation}
    \braket{\phi|\psi} = \bra{\phi}\left(\int \ket{\vec{r}}\bra{\vec{r}}d^3r\right)\ket{\psi} = \int \braket{\vec{r}|\phi}^*\braket{\vec{r}|\psi} d^3r = \int \phi^*(\vec{r})\psi(\vec{r}) d^3r
\end{equation}

Since $\vec{R}\ket{\vec{r}} = \vec{r}\ket{\vec{r}}$, we can write:
\begin{equation}
    \vec{R}^{\, n}\ket{\vec{r}} = \vec{r}\, ^n\ket{\vec{r}} \to \bra{\vec{r}\ '}\vec{R}^{\, n}\ket{\vec{r}} = \bra{\vec{r}\ '}\vec{r}\, ^n\ket{\vec{r}} \to \bra{\vec{r}\ '}\vec{R}^{\, n}\ket{\vec{r}} = \vec{r}\, ^n\delta(\vec{r} - \vec{r}\ ')
\end{equation}

Note that the operator $\vec{R}$ is Hermitian, since:
\begin{equation}
    \braket{\phi|\vec{R}\,|\psi} = \int \braket{\vec{r}|\phi}^*\vec{r}\braket{\vec{r}|\phi} = \left[\int \braket{\vec{r}|\phi}\vec{r}\braket{\vec{r}|\phi}^*\right]^* = \braket{\psi|\vec{R}\,|\phi}^*
\end{equation}

\subsection{Momentum representation of state space}

The basis $\{\vec{p}\}$ of the momentum representation is made of the eigenkets of the momentum operator $\vec{P}$:
\begin{equation}
    \vec{P}\ket{\vec{p}} = \vec{p}\ket{\vec{p}}
\end{equation}
where $\vec{p}$, the momentum vector, is the eigenvalue of the momentum operator $\vec{P}$. The orthonormality and completeness relations are given by\footnote{Remember that, as in the position representation, elements of an uncountable set of functions such as this \textit{cannot} be elements of $\F$.}:
\begin{equation}
    \braket{\vec{p}|\vec{p}\ '} = \delta(\vec{p} - \vec{p}\ ')
\end{equation}
\begin{equation}
    \int \ket{\vec{p}}\bra{\vec{p}} d^3p = \Imat
\end{equation}
And every state vector $\ket{\psi}$ can be expanded in terms of the momentum eigenkets $\ket{\vec{p}}$ as:
\begin{equation}
    \ket{\psi} = \int \braket{\vec{p}|\psi} \ket{\vec{p}} d^3 p = \int \Psi(\vec{p}) \ket{\vec{p}} d^3 p
\end{equation}
where the expansion coefficient $\Psi(\vec{p}) = \braket{\vec{p}|\psi}$ is the \textbf{momentum space wave function}. The quantity $|\braket{\vec{p}|\psi}|^2d^3 p = |\Psi(\vec{p})|^2d^3p$ represents the probability of finding the system's momentum inside the volume element $d^3p$ located between $\vec{p}$ and $\vec{p} + d\vec{p}$.

The scalar product between two state vectors $\ket{\psi}$ and $\ket{\phi}$ can be written as:
\begin{equation}
    \braket{\phi|\psi} = \bra{\phi}\left(\int \ket{\vec{p}}\bra{\vec{p}}d^3p\right)\ket{\psi} = \int \braket{\vec{p}|\phi}^*\braket{\vec{p}|\psi} d^3p = \int \Phi^*(\vec{p})\Psi(\vec{p}) d^3p
\end{equation}

Since $\vec{P}\ket{\vec{p}} = \vec{p}\ket{\vec{p}}$, we can write:
\begin{equation}
    \vec{P}^{\, n}\ket{\vec{p}} = \vec{p}\, ^n\ket{\vec{p}} \to \bra{\vec{p}\ '}\vec{P}^{\, n}\ket{\vec{p}} = \bra{\vec{p}\ '}\vec{p}\, ^n\ket{\vec{p}} \to \bra{\vec{p}\ '}\vec{P}^{\, n}\ket{\vec{p}} = \vec{p}\, ^n\delta(\vec{p} - \vec{p}\ ')
\end{equation}

\subsection{Connecting position and momentum representations}

When changing from the $\{\vec{r}\}$ basis to the $\{\vec{p}\}$ basis, we encounter a transformation function $\braket{\vec{r}|\vec{p}}$. To find the expression for this transformation, let us establish the following relations:

\begin{equation}
    \braket{\vec{r}|\psi} = \bra{\vec{r}}\left(\int \ket{\vec{p}}\bra{\vec{p}}d^3p\right)\ket{\psi} = \int \braket{\vec{r}|\vec{p}}\braket{\vec{p}|\psi}\ d^3p = \int \braket{\vec{r}|\vec{p}}\Psi(\vec{p})\ d^3p
\end{equation}
which, as $\psi(\vec{r}) = \braket{\vec{r}|\psi}$, means:
\begin{equation} \label{pos_mom}
    \psi(\vec{r}) = \int \braket{\vec{r}|\vec{p}}\Psi(\vec{p})\ d^3p
\end{equation}
Similarly, we find:
\begin{equation}
    \Psi(\vec{p}) = \braket{\vec{p}|\psi} = \bra{\vec{p}}\left(\int \ket{\vec{r}}\bra{\vec{r}}d^3r\right)\ket{\psi} = \int \braket{\vec{p}|\vec{r}}\braket{\vec{r}|\psi} d^3r = \int \braket{\vec{p}|\vec{r}}\psi(\vec{r})\ d^3r
\end{equation}
\begin{equation} \label{mom_pos}
    \Psi(\vec{p}) = \int \braket{\vec{p}|\vec{r}}\psi(\vec{r})\ d^3r
\end{equation}

\textbf{Equation \ref{pos_mom}} and \textbf{Equation \ref{mom_pos}} imply that $\phi(\vec{r})$ and $\Phi(\vec{p})$ are Fourier transforms of each other. In quantum mechanics, the Fourier transform of a function $f(\vec{r})$ is given as:
\begin{equation}
    f(\vec{r}) = \frac{1}{(2\pi\hbar)^{3/2}}\int e^{i\vec{p}\cdot \vec{r}/\hbar} g(\vec{p})\ d^3p
\end{equation}
Hence, the function $\braket{\vec{r}|\vec{p}}$ is given by:
\begin{equation} \label{transform_1}
    \braket{\vec{r}|\vec{p}} = \frac{1}{(2\pi\hbar)^{3/2}}e^{i\vec{p}\cdot \vec{r}/\hbar}
\end{equation}

This function transforms from the momentum to the position representation. The function corresponding to the inverse transformation, $\braket{\vec{p}|\vec{r}}$, is given by:
\begin{equation}
    \braket{\vec{p}|\vec{r}} = \braket{\vec{r}|\vec{p}}^* = \frac{1}{(2\pi\hbar)^{3/2}}e^{-i\vec{p}\cdot \vec{r}/\hbar}
\end{equation}
The quantity $|\braket{\vec{r}|\vec{p}}|^2 = \frac{1}{(2\pi\hbar)^3}$ represents the probability of finding the system at the point $\vec{r}$ when its momentum is $\vec{p}$.

\subsubsection{Momentum operator in the position representation}

To determine the expression for the momentum operator $\vec{P}$ in the position representation, let us calculate:
\begin{equation}
    \braket{\vec{r}|\vec{P}|\psi} = \int \braket{\vec{r}|\vec{P}|\psi}d^3p \stackrel{\footnotemark}{=}\footnotetext{Here, we apply the closure relation.} \int \braket{\vec{r}|\vec{P}|\vec{p}}\braket{\vec{p}|\psi}d^3p \stackrel{\footnotemark}{=}\footnotetext{Here, we have used that $\vec{P}\ket{\psi} = \vec{p}\ket{\vec{p}}$.} \int \vec{p}\braket{\vec{r}|\vec{p}}\braket{\vec{p}|\psi}d^3p \stackrel{\footnotemark}{=}\footnotetext{Here, we have used the definition of the transformation function from the momentum to the position representation, defined in \textbf{Equation \ref{transform_1}}.} \frac{1}{(2\pi\hbar)^{3/2}} \int \vec{p}e^{i\vec{p}\cdot \vec{r}/\hbar}\Psi (\vec{p})d^3p
\end{equation}
Now, since $\vec{p}e^{i \vec{p}\cdot \vec{r}/\hbar} = -i\hbar \vec{\nabla}e^{i \vec{p}\cdot \vec{r}/\hbar}$, and using \textbf{Equation \ref{transform_1}}, we can write:
\begin{equation}
    \braket{\vec{r}|\vec{P}|\psi} = -i\hbar \vec{\nabla} \left( \frac{1}{(2\pi\hbar)^{3/2}}\int e^{i\vec{p}\cdot \vec{r}/\hbar}\Psi (\vec{p})d^3p\right) = -i\hbar \vec{\nabla} \left(\int \braket{\vec{r}|\vec{p}}\braket{\vec{p}|\psi} d^3p\right) = -i\hbar \vec{\nabla} \braket{\vec{r}|\psi} 
\end{equation}
So, the momentum operator acting on the state $\ket{\psi}$ is expressed in the position representation as the differential operator:
\begin{equation}
    \vec{P} = -i\hbar \vec{\nabla}
\end{equation}
acting on the wave function $\psi(\vec{r}) = \braket{\vec{r}|\psi}$. Thus, the momentum operator is represented by the differential operator $-i\hbar \vec{\nabla}$ in the position representation.

\subsubsection{Canonical commutation relations}

Calculating the separate terms of the $x$-component commutator of the operators $\vec{R}$ and $\vec{P}$ in the position representation, we find:
\begin{equation}
    XP_x\psi(\vec{r}) = -i\hbar x \frac{\partial \psi(\vec{r})}{\partial x}
\end{equation}
\begin{equation}
    P_xX\psi(\vec{r}) = -i\hbar \frac{\partial}{\partial x}(x\psi(\vec{r})) = -i\hbar \psi(\vec{r}) - i \hbar x \frac{\partial \psi(\vec{r})}{\partial x}
\end{equation}
so that:
\begin{equation}
    [X,P_x]\psi(\vec{r}) = XP_x\psi(\vec{r}) - P_xX\psi(\vec{r}) = -i\hbar x \frac{\partial \psi(\vec{r})}{\partial x} + i\hbar \psi(\vec{r}) + i\hbar x \frac{\partial \psi(\vec{r})}{\partial x} = i\hbar \psi(\vec{r})
\end{equation}
Therefore:
\begin{equation}
    [X,P_x] = i\hbar
\end{equation}
Likewise, it can be shown for the $y$ and $z$ components:
\begin{equation}
    [Y,P_y] = i\hbar, \qquad [Z,P_z] = i\hbar
\end{equation}

We can also check that, crossing different components:
\begin{equation}
    [X, P_y] = [X, P_z] = [Y, P_x] = [Y, P_z] = [Z, P_x] = [Z, P_y] = 0
\end{equation}

With this, and the fact that the three degrees of freedom are independent, we arrive at the \textbf{canonical commutation relations}:
\begin{equation}
    [X_j, P_k] = i\hbar \delta_{jk}, \quad [X_j, X_k] = [P_j, P_k] = 0, \quad j,k = x,y,z
\end{equation}

It is important to note that, even though the particular expression of an operator in different representations may vary, the commutation relations for operators are representation independent\footnote{\color{red} Add proof}.

\subsection{Tensor product of two vector spaces}

Up to this point, we have always talked about the wave functions of a single particle. However, even in that case, one can consider a one-dimensional, two-dimensional or even three-dimensional wave function. It is clear that the particularisation of the general state space $\F$ is not the same for functions of one variables (we will call the 1D state space $\varepsilon_x$) as for functions of three variables (we will call the 3D state space, $\varepsilon_{\vec{r}}$). $\varepsilon_{\vec{r}}$ and $\varepsilon_x$ are therefore different spaces. Nevertheless, $\varepsilon_{\vec{r}}$ appears to be essentially a generalization of $\varepsilon_x$. Does there exist a more precise relation between these two spaces?

In this section, we are going to define and study the operation of taking the tensor product of vector spaces, and apply it to state spaces. This will answer, in particular, the question we have just asked: $\varepsilon_{\vec{r}}$ can be constructed from $\varepsilon_x$ and another two spaces, $\varepsilon_y$ and $\varepsilon_z$, which are isomorphic to it\footnote{\color{red}clarify what this means}. 

We give the following definition of the tensor product of two vector spaces:

\begin{definition}
    Let $\varepsilon_1$ and $\varepsilon_2$ be two\footnote{This definition can easily be extended to a finite number of vector spaces.} vector spaces, of (finite or infinite) dimension $N_1$ and $N_2$, respectively. Let vectors and operators belonging to $\varepsilon_1$ and $\varepsilon_2$ be denoted by an index $(1)$ or $(2)$, respectively. The vector space $\varepsilon = \varepsilon_1 \otimes \varepsilon_2$ is called the \textit{tensor product of} $\varepsilon_1$ \textit{and} if there is associated with each pair of vectors, $\ket{\psi(1)}$ belonging to $\varepsilon_1$ and $\ket{\psi(2)}$ belonging to $\varepsilon_2$, a vector belonging to $\varepsilon$, denoted by\footnote{This vector can be written as $\ket{\psi(1)}\otimes\ket{\psi(2)}$ or $\ket{\psi(2)}\otimes\ket{\psi(1)}$; the order is of no importance.}:
    \begin{equation}
        \ket{\psi(1)}\otimes\ket{\psi(2)} = \ket{\psi(1)\psi(2)} 
    \end{equation}
    which is called the tensor product of $\ket{\psi(1)}$ and $\ket{\psi(2)}$, and which satisfies the following conditions:
    \begin{itemize}
        \item It is \textit{linear} with respect to multiplication by complex numbers:
        \begin{equation}
            \alpha\left[\ket{\psi(1)}\otimes\ket{\psi(2)}\right] = \left[\alpha\ket{\psi(1)}\right]\otimes\ket{\psi(2)} = \ket{\psi(1)}\otimes(\alpha\ket{\psi(2)})
        \end{equation}
        \item It is \textit{distributive} with respect to vector addition:
        \begin{equation}
            \begin{split}
                \ket{\psi(1)}\otimes \left[\ket{\chi_1(2)} + \ket{\chi_2(2)}\right] &= \ket{\psi(1)}\otimes\ket{\chi_1(2)} + \ket{\psi(1)}\otimes\ket{\chi_2(2)}\\
                \left[\ket{\psi_1(1)} + \ket{\psi_2(1)}\right]\otimes\ket{\chi(2)} &= \ket{\psi_1(1)}\otimes\ket{\chi(2)} + \ket{\psi_2(1)}\otimes\ket{\chi(2)}   
            \end{split}
        \end{equation}
        \item When a basis has been chosen for $\varepsilon_1$, $\{u_i(1)\}$, and for $\varepsilon_2$, $\{u_j(2)\}$, the set of vectors $\ket{u_i(1)}\otimes\ket{v_l(2)}$ constitutes a basis in $\varepsilon$. If $N_1$ and $N_2$ are finite, the dimensions of $\varepsilon$ is $N_1N_2$.
    \end{itemize}
\end{definition}

\subsection{Vectors in the tensor product space}

Let us consider a tensor product $\varepsilon = \varepsilon_1\otimes\varepsilon_2$. Let us also consider a vector $\ket{\psi(1)}\otimes\ket{\chi(2)}$ in $\varepsilon$. Whatever $\ket{\psi(1)}$ and $\ket{\chi(2)}$ may be, they can be expressed in the basis of their respective spaces, $\{\ket{u_i(1)}\}$ and $\{\ket{v_l(2)}\}$, as:
\begin{equation}
    \ket{\psi(1)} = \sum_i a_i\ket{u_i(1)}, \qquad \ket{\chi(2)} = \sum_l b_l\ket{v_l(2)}
\end{equation}
The expansion of the vector $\ket{\psi(1)}\otimes\ket{\chi(2)}$ in the basis $\{\ket{u_i(1)}\otimes \ket{v_l(2)}\}$ of $\varepsilon$ is then given by:
\begin{equation} \label{tensor_product_vector}
    \ket{\psi(1)}\otimes\ket{\chi(2)} = \sum_{i,l} a_ib_l\ket{u_i(1)}\otimes\ket{v_l(2)}
\end{equation}

Therefore, we obtain the following result:
\begin{definition}
    The components of a tensor product vector $\ket{\psi(1)}\otimes\ket{\chi(2)}$ are the products of the components of the two vectors of the product $\ket{\psi(1)}$ and $\ket{\chi(2)}$.
\end{definition}

Note that a general vector in the tensor product space $\varepsilon$ can be written as:
\begin{equation} \label{general_tensor_product_vector}
    \ket{\psi} = \sum_{i,l} c_{i,l}\ket{u_i(1)}\otimes\ket{v_l(2)}
\end{equation}


Thus, by comparing this expression with \textbf{Equation \ref{tensor_product_vector}}, we can see that there exist vectors in $\varepsilon$ that cannot be written as a tensor product of two vectors belonging to $\varepsilon_1$ and $\varepsilon_2$. This is because, in general, there are many cases where $c_{i,j}$ cannot be written as a product of two numbers $a_i$ and $b_j$. However, any arbitrary vector of $\varepsilon$ can always be decomposed into a \textit{linear combination} of tensor product vectors belonging to $\varepsilon_1$ and $\varepsilon_2$.

\subsection{Scalar product in the tensor product space}

The existence of the scalar product in $\varepsilon_1$ and $\varepsilon_2$ allows us to define one in $\varepsilon$ too.

\begin{definition}
    The scalar product of $\ket{\psi(1)\chi(2)} = \ket{\psi(1)}\otimes\ket{\chi(2)}$ and $\ket{\psi'(1)\chi'(2)} = \ket{\psi'(1)}\otimes\ket{\chi'(2)}$ is defined as:
    \begin{equation}
        \braket{\psi'(1)\chi'(2)|\psi(1)\chi(2)} = \braket{\psi'(1)|\psi(1)}\braket{\chi'(2)|\chi(2)}
    \end{equation}
    Notice, in particular, that the basis $\{\ket{u_i(1)v_l(2)} = \ket{u_i(1)}\otimes\ket{v_l(2)}\}$ of $\varepsilon$ is orthonormal if each of the bases of $\varepsilon_1$ and $\varepsilon_2$ is orthonormal:
    \begin{equation}
        \braket{u_{i'}(1)v_{l'}(2)|u_i(1)v_l(2)} = \braket{u_{i'}(1)|u_{i}(1)}\braket{v_{l'}(2)|v_l(2)} = \delta_{ii'}\delta_{ll'}
    \end{equation}
\end{definition}

\subsection{Tensor product of operators}

We define operators inside the tensor product space in the following way:

\begin{definition} \label{tensor_product_operator}
    Consider a linear operator $A(1)$ defined in $\varepsilon_1$. We associate it with a linear operator $\tilde{A}(1)$ defined in $\varepsilon$, which we call the extension of $A(1)$ in $\varepsilon$, and which is characterized in the following way: when $\tilde{A}(1)$ acts on a vector $\ket{\psi(1)}\otimes\ket{\chi(2)}$ of $\varepsilon$, one obtains, by definition:
    \begin{equation}
        \tilde{A}(1)\left[\ket{\psi(1)}\otimes\ket{\chi(2)}\right] = \left[A(1)\ket{\psi(1)}\right]\otimes\ket{\chi(2)}
    \end{equation}
    We obtain an analogous definition for an operator $\tilde{B}(2)$ initially defined in $\varepsilon_2$:
    \begin{equation}
        \tilde{B}(2)\left[\ket{\psi(1)}\otimes\ket{\chi(2)}\right] = \ket{\psi(1)}\otimes\left[B(2)\ket{\chi(2)}\right]
    \end{equation}
\end{definition}
Then, the action of $\tilde{A}(1)$ on an arbitrary vector of $\varepsilon$ is given by:
\begin{equation}
    \tilde{A}(1)\ket{\psi} = \tilde{A}(1)\left(\sum_{i,l} c_{i,l}\ket{u_i(1)}\otimes\ket{v_l(2)}\right) = \sum_{i,l} c_{i,l}\left[A(1)\ket{u_i(1)}\right]\otimes\ket{v_l(2)}
\end{equation}
and the action of $\tilde{B}(2)$ is given by:
\begin{equation}
    \tilde{B}(2)\ket{\psi} = \tilde{B}(2)\left(\sum_{i,l} c_{i,l}\ket{u_i(1)}\otimes\ket{v_l(2)}\right) = \sum_{i,l} c_{i,l}\ket{u_i(1)}\otimes\left[B(2)\ket{v_l(2)}\right]
\end{equation}

Therefore, we obtain:
\begin{definition} \label{tensor_product_operator_2}
    Let $A(1)$ and $B(2)$ be two linear operators acting, respectively, in $\varepsilon_1$ and $\varepsilon_2$. Their tensor product $A(1)\otimes B(2)$ is the linear operator acting in $\varepsilon$, defined by the following relation which describes its action on the tensor product vectors:
    \begin{equation}
        \left[A(1)\otimes B(2)\right]\left[\ket{\psi(1)}\otimes\ket{\chi(2)}\right] = \left[A(1)\ket{\psi(1)}\right]\otimes\left[B(2)\ket{\chi(2)}\right]
    \end{equation}
\end{definition}

\textbf{Comment on extension of operators:} The extensions of operators are special cases of tensor products. If $\Imat(1)$ and $\Imat(2)$ are the identity operators in $\varepsilon_1$ and $\varepsilon_2$, respectively, then:
\begin{equation}
    \begin{split}
        \tilde{A}(1) &= A(1)\otimes \Imat(2)\\
        \tilde{B}(2) &= \Imat(1)\otimes B(2)
    \end{split}
\end{equation}
By combining these expressions with \textbf{Definition \ref{tensor_product_operator_2}}, it is now very easy to deduce the expressions in \textbf{Definition \ref{tensor_product_operator}}.

It is also easy to show that any pair of such operators commute in $\varepsilon$, by looking at the action of $\tilde{A}(1)\tilde{B}(2)$ and $\tilde{B}(2)\tilde{A}(1)$ on an arbitrary vector of the basis of $\varepsilon$:
\begin{equation}
    \left.
        \begin{split}
            \tilde{A}(1)\tilde{B}(2)\ket{u_i(1)}\otimes\ket{v_l(2)} &= \tilde{A}(1)\left[\ket{u_i(1)}\otimes\left[B(2)\ket{v_l(2)}\right]\right] =\\
            &= \left[A(1)\ket{u_i(1)}\right]\otimes\left[B(2)\ket{v_l(2)}\right] \\
            \tilde{B}(2)\tilde{A}(1)\ket{u_i(1)}\otimes\ket{v_l(2)} &= \tilde{B}(2)\left[\left[A(1)\ket{u_i(1)}\right]\otimes\ket{v_l(2)}\right] =\\
            &= \left[A(1)\ket{u_i(1)}\right]\otimes\left[B(2)\ket{v_l(2)}\right]
        \end{split}
    \ \right\} \longrightarrow \tilde{A}(1)\tilde{B}(2) = \tilde{B}(2)\tilde{A}(1)
\end{equation}

As the result of the action of both operator products on the basis vectors is the same, the operators commute:
\begin{equation}
    [\tilde{A}(1), \tilde{B}(2)] = 0
\end{equation}

% NOTES: 

% Complete set of commuting observables CSCO
% Pauli matrices
% RASHBA no