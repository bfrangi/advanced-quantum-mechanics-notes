\section*{Orthogonal Projection}

Computing the orthogonal projection of a vector $v\in V$ onto a subspace $E \subset V$ is finding the closest approximation of $v$ in $E$. The matrix of the projection onto the subspace $E$ is $P_E$ such that $P_Ev\in E$, $\forall v\in V$.\\

Any vector $v\in V$ can be expressed as the sum of its orthogonal projections onto the subspaces $E$ and $E^\perp$:

$$v=P_Ev+P_{E^\perp}v$$

From this, we have:

$$P_E+P_{E^\perp}=\Imat$$

The matrix $P$ of an orthogonal projection always satisfies $P=P^*$ and $P^2=P$.\\

\textit{Proof:}

For any two vectors $x,\;y\in V$, we can decompose them into the sum of the orthogonal projections onto the subspaces $E\subset V$ and $E^\perp \subset V$ $(P=P_E)$:

$$x=P_E\,x+P_{E^\perp}\,x,\text{ }\text{ }\text{ }y=P_E\,y+P_{E^\perp}\,y$$

We denote $x_E=P_E\,x\in E$, $\text{ }\text{ }x_{E^\perp}=P_{E^\perp}\,x \perp E$, $\text{ }\text{ }y_E=P_E\,y \in E$, $\text{ }$ and $\text{ }\text{ }y_{E^\perp}=P_{E^\perp}\,y\perp E$. Then:

$$
    \begin{cases}
      (P\,x,\;y)=(x_E,\;y)=(x_E,\;y_E+y_{E^\perp})=(x_E,\;y_E)+(x_E,\;y_{E^\perp})=(x_E,\;y_E)\\
      (x,\;P\,y)=(x,\;y_E)=(x_E+x_{E^\perp},\;y_E)=(x_E,\;y_E)+(x_{E^\perp},\;y_E)=(x_E,\;y_E)
    \end{cases}
$$

So, $(P\,x,\;y)=(x,\;P\,y)\Leftrightarrow(P\,x,\;y)=(P^*\,x,\;y)\Leftrightarrow P=P^*$.\\

Then, if we take $x_E=Px\in E$:

$$Px_E=x_E\Leftrightarrow P(Px)=Px \Leftrightarrow P^2x=Px\Leftrightarrow P^2=P$$

\subsubsection{Matrix of the Orthogonal Projection}

We can compute the orthogonal projection by means of the following formula (where $v_i$ is an orthogonal basis of the subspace $E$, for all $\naturalset{i}{k}$):

$$P_Ev=\sum_{i=1}^k \frac{(v,\;v_i)}{||v_i||^2}v_i$$

Form this:

$$P_Ev=\sum_{i=1}^k\frac{v_i^*v}{||v_i||^2}v_i=\sum_{i=1}^k\frac{v_iv_i^*}{||v_i||^2}v$$

Dividing by $v$, the matrix $P_E$ of the orthogonal projection onto the subspace $E$, given an orthogonal basis of $E$, $\vectorset{B}{v}{k}$ is:

$$P_E=\sum_{i=1}^k\frac{v_iv_i^*}{||v_i||^2}$$

\subsubsection{Gram-Schmidt Algorithm}

In order to orthogonalize a vector $x_i$ with respect to a basis of $E_{i-1}$, we use the following algorithm:

$$v_i=x_i-P_{E_{i-1}}x_i\rightarrow v_i \perp E_{i-1}$$

\subsubsection{Least Squares Solution}

In order to give an approximation to $Ax=b$ when the system has no solution (which means $b\notin RanA$), we need to find the vector in $RanA$ that is closest to $b$. Therefore, we need to orthogonally project $b$ onto the subspace $RanA$.\\

The approximation to $Ax=b$ will be the solution of the system $Ax=P_{RanA}b$, which will necessarily have a solution, because $P_{RanA}b \in RanA$. In order to find $P_{RanA}b$, if we have an orthogonal basis $\vectorset{B_{RanA}}{v}{k}$ of $RanA$, we can use the formula:

$$P_{RanA}v=\sum_{i=1}^k\frac{(b,\;v_i)}{||v_i||^2}v_i$$

Alternatively, we can use normal equations:\\

We know that $b-P_{RanA}b \perp RanA$. So, if $\vectorset{A}{a}{k}$, then $(b-P_{RanA}b,\;a_i)=0$, $\forall \naturalset{i}{k}$. With $P_{RanA}b=Ax$:

$$a_i^*(b-Ax)=0,\text{ }\text{ }\forall \naturalset{i}{k} \rightarrow A^*(b-Ax)=0 \rightarrow A^*b-A^*Ax=0 \rightarrow$$

$$\rightarrow A^*b=A^*Ax \rightarrow x=(A^*A)^{-1}A^*b \rightarrow Ax=A(A^*A)^{-1}A^*b$$

Therefore, the matrix of the orthogonal projection onto the subspace $RanA$ is $P_{RanA}=A(A^*A)^{-1}A^*$, given any basis of $RanA$ (any matrix with the same range as $A$).