\section{Mathematical Tools of Quantum Mechanics. Postulates of Quantum Mechanics}

\subsection{Hilbert Spaces}

\textbf{Hilbert spaces}, also known as \textbf{complex vector spaces}, are defined in \textbf{Definition \ref{hilbert_spaces}}.

\begin{definition} \label{hilbert_spaces}
    A Hilbert space $\Hbt$ consists of a set of vectors $\psi$, $\psi$, $\chi$, ... and a set of scalars $a$, $b$, $c$, ... which satisfy the following four properties:

    \begin{enumerate}
        \item \textbf{$\Hbt$ is a linear space}\footnote{See \textbf{Definition \ref{linear_vector_space}} for the definition of linear vector space.}.
        \item \textbf{$\Hbt$ has a defined scalar product that is strictly positive}. The scalar product of an element $\psi$ with another element $\phi$ is in general a complex number, denoted by $(\psi,\ \phi)$. The scalar product satisfies the following properties\footnote{\textbf{Note:} Watch out for the order! Since the scalar product is a complex number, the quantity $(\psi,\ \phi) = \psi^*\phi$ is generall not equal to $(\phi,\ \psi) = \phi^*\psi$.}:
        \begin{equation}
            (\psi,\ \phi) = (\phi,\ \psi)^*
        \end{equation}
        \begin{equation}
            (\phi,\ a\psi_1 + b\psi_2) = a(\phi,\ \psi_1) + b(\phi,\ \psi_2)
        \end{equation}
        \begin{equation}
            (a\phi_1 + b\phi_2,\ \psi) = a^*(\phi_1,\ \psi) + b^*(\phi_2,\ \psi)
        \end{equation}
        \begin{equation}
            (\psi,\ \psi) = ||\psi||^2 \geq 0 \text{ (the equality holds only for } \psi = 0\text{)}
        \end{equation}
        \item \textbf{$\Hbt$ is separable}.
        \item \textbf{$\Hbt$ is complete}.
    \end{enumerate}
\end{definition}

We should note that in a scalar product $(\phi, \ \psi)$, the second factor, $\psi$, belongs to the Hilbert space $\Hbt$, while the first factor, $\phi$, belongs to its dual Hilbert space $\Hbt^*$\footnote{More on the dual space in the next section.}. The distinction between $\Hbt$ and $\Hbt^*$ is due to the fact that, as mentioned above, the scalar product is not commutative: $(\psi, \ \phi) \neq (\phi, \ \psi)$; the order matters!

\subsection{The dual space} \label{dualspace}

Given any Hilbert space $\Hbt$, one can construct another complex vector space $\Hbt^*$, called the \textbf{dual vector space}. It contains all the linear functionals in $\Hbt$, which are a special kind of operator that maps all elements of $\Hbt$ onto complex numbers\footnote{See Appendix \textbf{Section \ref{linfunct}} for more on linear functionals.}. In general, for an abstract vector space $\Hbt$:

\begin{definition}
    Given a Hilbert space $\Hbt$, the dual space $\Hbt^*$ is the vector space of all linear functionals in $\Hbt$.
\end{definition}

Therefore, all linear functionals $L: \Hbt\to \C$ live in $\Hbt^*$ ($L\in V^*$).

The reason that the dual space is so interesting for quantum mechanics is that our goal as quantum physicists is to build a mathematical model for the real world, and in the end we want to be able to extract useful values and predictions from this model. For example, we may want to know the probability of getting a certain energy; or the average position we expect in a certain state. All these are scalar values, that we need to extract from a quantum state $\ket{\psi}$, so we know we will need a linear functional someplace or other!

This may all sound really abstract at first glance, but hopefully it will become a lot clearer in the next section when we look at the \textbf{Dirac notation}.

\subsection{Dirac Notation}

In quantum mechanics, we use the Dirac notation to represent wave functions:

\begin{itemize}
    \item We call the elements of $\Hbt$ ``ket'' vectors, and we represent them as $\ket{\psi}\in\Hbt$.
    \item We call the elements of $\Hbt^*$ ``bra'' vectors, and we represent them as $\bra{\phi}\in\Hbt^*$.
\end{itemize}

Bra vectors are operators that linearly map elements of $\Hbt$ into complex numbers:

\begin{equation}
    \begin{matrix}
    \bra{\phi}: & \Hbt & \rightarrow & \C\\
    \bra{\phi}: & \ket{\psi} & \rightarrow & \braket{\phi}{\psi}\\
    \end{matrix}
\end{equation}

\subsubsection{Inner product and bra-ket notation}

Notice that, when we put a bra and a ket together ($\bra{\phi}\ket{\psi}$), they look suspiciously like an inner product in this notation: $\braket{\phi}{\psi}$. If we go back at how our $L_x$ operator in $\R^2$ acts on a column vector:

\begin{equation}
    L_x\begin{bmatrix}
        a \\ b
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 \\
    \end{bmatrix}\cdot
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = 1 \cdot a + 0\cdot b = a
\end{equation}

Notice that its action is the same as if we were taking the dot product with the $x$ unit vector:

\begin{equation}
    x\cdot\begin{bmatrix}
        a \\ b
    \end{bmatrix} = 
    \begin{bmatrix}
        1 \\ 0
    \end{bmatrix}\cdot
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = 1 \cdot a + 0\cdot b = a
\end{equation}

In fact, when a linear functional in $\R^n$ acts on any vector, it can be written equivalently as a dot product with the corresponding column vector:

\begin{equation}
    L_x \vec{v} = L_x^T\cdot \vec{v}
\end{equation}

This is actually a very general mathematical fact, rooted within something called the \textbf{Riesz Representation Theorem}:

\begin{theorem}
    \textbf{(Riesz Representation Theorem)} For any linear functional $L_\phi$, the action of $L_\phi$ is equivalent to taking the inner product with some unique vector $\vec{\phi}$.
\end{theorem}

In our example of $L_x$, we have that $\vec{\phi} = \vec{x} = [1\ 0]^T$:

\begin{equation}
    L_x\vec{v} = \begin{bmatrix}
        1 \\ 0
    \end{bmatrix}\cdot \vec{v}    
\end{equation}

This is the reason for the suggestive notation for bra vectors: they are operators whose action on a ket is mathematically equivalent to taking the inner product with said ket:
\begin{equation}
    \bra{\phi}\ket{\psi} = \braket{\phi}{\psi}
\end{equation}

That is the power of bra-ket notation: it has the Riesz Representation Theorem baked right into it. Whatever you do, breaking apart inner products and putting together bras and kets, you will always have something that makes mathematical sense. Although bra and the inner product are two entities that are completely different mathematically, the bra-ket notation makes their connection completely seamless, thanks to the Riesz Representation Theorem.

% https://www.youtube.com/watch?v=lRR-qgjaKlg

\subsubsection{Properties of bras and kets}

Some properties that arise naturally from the Dirac notation:

\begin{equation}
    \braket{\psi}{\lambda_1\phi_1+\lambda_2\phi_2} = \lambda_1\braket{\psi}{\phi_1} + \lambda_2\braket{\psi}{\phi_2}
\end{equation}
\begin{equation}
    \braket{\lambda_1\psi+\lambda_2\psi_2}{\phi} = \lambda_1^*\braket{\psi_1}{\phi} + \lambda_2^*\braket{\psi_2}{\phi}
\end{equation}
\begin{equation}
    \braket{\psi}{\phi} = \braket{\phi}{\psi}^*
\end{equation}
\begin{equation}
    \braket{\psi}{\psi}\text{ is real, positive and only zero if }\ket{\psi} = 0
\end{equation}


\subsection{Linear operators}

A linear map is defined as:

\begin{definition} \label{linear_map}
    A linear map (or linear operator) is a mathematical entity $A$ that associates a function with another function such that:
    \begin{equation}
        A (\lambda_1\psi_1 + \lambda_2\psi_2) = \lambda_1 A\psi_1 + \lambda_2 A\psi_2
    \end{equation}
\end{definition}


In the quantum mechanical context, we can see them as entities that transform a ket into another ket. Some example linear operators are:
\begin{itemize}
    \item \textbf{Commutator:} The commutator of two operators $A$ and $B$ is defined as:
    \begin{equation}
        [A, B] \equiv AB - BA
    \end{equation}
    Two operators are said to commute if their commutator is equal to zero, and hence $AB = BA$. See \textbf{Section \ref{uncertainty_relation}} to see an interesting application of commutator algebra for finding the uncertainty products of two operators.
    \item \textbf{Anti-commutator:} The anti-commutator of two operators $A$ and $B$ is defined as:
    \begin{equation}
        \{A, B\} \equiv AB + BA
    \end{equation}
    \item \textbf{Projector:} $P_\phi = \ket{\phi}\bra{\phi}$. The projector operator $P_\phi$ acting on a ket $\ket{\psi}$ gives a new ket that is proportional to $\ket{\phi}$. The coefficient of proportionality is the scalar product $\braket{\phi}{\psi}$.\footnote{\textit{Proof:} $P_\phi\ket{\psi} = \ket{\phi}\bra{\phi}\ket{\psi} = \ket{\phi}\braket{\phi}{\psi} = \braket{\phi}{\psi}\ket{\phi}$.}
    \item \textbf{Inverse:} assuming it exists, the inverse operator $A^{-1}$ of the operator $A$, when applied to $A$, gives the identity operator. Also, $A$ is the inverse of $A^{-1}$, so that $AA^{-1} = A^{-1}A = \Imat$.
    \item \textbf{Hermitian conjugation:} the hermitian conjugate (or adjoint) $A^\dagger$ of an operator $A$ is obtained by interchanging the columns of the operator by its rows, and taking the complex conjugate of all elements. For example:
    \begin{equation}
        A = \begin{bmatrix}
            i & 1 \\
            3-i & -i
        \end{bmatrix}\rightarrow
        A^\dagger = \begin{bmatrix}
            -i & 3 + i \\
            1 & i
        \end{bmatrix}
    \end{equation}
    Some properties of the adjoint are:
    \begin{enumerate}
        \item[a)] $\left(A^\dagger\right)^\dagger = A$.
        \item[b)] $\left(\lambda A\right)^\dagger = \lambda^*A^\dagger$.
        \item[c)] $\left(A + B\right)^\dagger = A^\dagger + B^\dagger$.
        \item[d)] $\left(A B\right)^\dagger = B^\dagger A^\dagger$.
        \item [e)] $\left(\ket{u}\bra{v}\right)^\dagger = \ket{v}\bra{u}$.\footnote{\textit{Proof:} $\bra{\phi}\left(\ket{u}\bra{v}\right)^\dagger\ket{\psi} = \left[\bra{\psi}\left(\ket{u}\bra{v}\right)\ket{\phi}\right]^* = \braket{\psi}{u}^*\braket{v}{\phi}^* = \braket{u}{\psi}\braket{\phi}{v} = \braket{\phi}{v}\braket{u}{\psi} = \bra{\phi}\left(\ket{v}\bra{u}\right)\ket{\psi}$}
    \end{enumerate}
    The adjoint of a bra is its ket, and the adjoint of a ket is its bra.
    To obtain the hermitian conjugate of an expression:
    \begin{enumerate}
        \item[a)] Replace constants with their complex conjugate: $\lambda \to \lambda^*$.
        \item[b)] Replace operators with their Hermitian conjugates: $A\to A^\dagger$.
        \item[c)] Replace kets with bras: $\ket{\phi}\to\bra{\phi}$.
        \item[d)] Replace bras with kets: $\bra{\phi}\to\ket{\phi}$.
        \item[e)] Reverse the order of factors: $A\ket{\phi} \to \bra{\phi}A^\dagger$.

    \end{enumerate}
\end{itemize}

A special case of linear operators are \textbf{unitary operators}:

\begin{definition}
    A linear operator $U$ is said to be unitary if its inverse $U^{-1}$ is equal to its adjoint $U^\dagger$, so that $U^{-1} = U^\dagger$ and $U^\dagger U = UU^\dagger = \Imat$.
\end{definition}

Another special case are \textbf{Hermitian operators}:

\begin{definition}
    An operator $A$ is said to be Hermitian if $A^\dagger = A$.
\end{definition}

and \textbf{anti-Hermitian operators}:
\begin{definition}
    An operator $A$ is said to be anti-Hermitian if $A^\dagger = -A$.
\end{definition}

An example of a Hermitian operator is the projector operator, as $P_\phi^\dagger = \left(\ket{\phi}\bra{\phi}\right)^\dagger = \ket{\phi}\bra{\phi} = P_\phi$.

\subsubsection{Expected value of an operator}

In order to define the expected value of an operator, we first need to define the \textbf{matrix element}:

\begin{definition}
    Let $\ket{\psi}, \ \ket{\phi}$ be two kets, we call the matrix element of an operator $A$ between $\ket{\psi}$ and $\ket{\phi}$ the quantity $\bra{\psi}(A\ket{\phi})$. 
\end{definition}

Note that the matrix element of an operator $A$ between $\ket{\psi}$ and $\ket{\phi}$ is a complex number, and it is equal to the scalar product of $\ket{\psi}$ with the ket $A\ket{\phi}$. If we now define the expected value of an operator:

\begin{definition}
    The expected value $\langle A \rangle_\psi$ of $A$ in the state $\ket{\psi}$ is defined as the matrix element of $A$ between $\ket{\psi}$ and itself:
    \begin{equation}
        \langle A \rangle_\psi = \braket{\psi|A|\psi}
    \end{equation}
\end{definition}

It is easy to see that, if $\psi$ is chosen to be an eigenvector of $A$, then the expected value of $A$ in the state $\ket{\psi}$ is equal to the eigenvalue $\lambda$ of $A$ corresponding to the eigenvector $\ket{\psi}$:
\begin{equation}
    \braket{\psi|A|\psi} = \bra{\psi} (A \ket{\psi}) = \bra{\psi} (\lambda \ket{\psi}) = \lambda\braket{\psi|\psi} = \lambda
\end{equation}

This means that, for an arbitrary vector $\phi$ expressed as a linear combination of eigenvectors $\psi_i$ of $A$:
\begin{equation}
    \ket{\phi} = \sum_i c_i\ket{\psi_i}
\end{equation}
we have:
\begin{equation*}
    \braket{\phi|A|\phi} = \sum_i\sum_j c_i^*c_j\braket{\psi_i|A|\psi_j} = \sum_i\sum_j c_i^*c_j\braket{\psi_i|\lambda_j|\psi_j} = \sum_i\sum_j c_i^*c_j\lambda_j\braket{\psi_i|\psi_j} = 
\end{equation*}
\begin{equation} \label{expected_value}
    = \sum_i\sum_j c_i^*c_j\lambda_j\delta_{ij} = \sum_i c_i^*c_i\lambda_i = \sum_i |c_i|^2\lambda_i
\end{equation}

\subsection{Closure relation}

For a set of vectors to form a basis of a Hilbert space $\Hbt$, they must fulfil the \textbf{closure relation} (also known as the completeness relation). In simple terms, if the set of vectors fulfills the closure relation, it means that with those vectors you can reach all possible directions in $\Hbt$, and any $\ket{\psi}\in\Hbt$ is a linear combination of those basis vectors. In our general Hilbert space:

\begin{definition}
    A set of vectors $\{\ket{A_1}, \ket{A_2}, ...\}$ in a Hilbert space $\Hbt$ form a basis for $\Hbt$ if and only if they fulfil the closure relation:
    \begin{equation}
        \Imat = \sum_i\ket{A_i} \bra{A_i}.
    \end{equation}
\end{definition}

Proof of this relation is given in \textbf{Section \ref{closure_relation_proof}}.

\subsection{Wave function space $\F$}

The wave function in quantum mechanics is an object whose modulus squared is a probability density function. If we look back at \textbf{Definition \ref{hilbert_spaces}}, we can see that, from a physical point of view, the set $\Hbt$ is clearly too wide in scope for our purposes. We need to restrict it to a subset of $\Hbt$ that is physically meaningful. This subset is called the \textbf{wave function space} $\F$, and it retains only the functions $\psi$ of $\Hbt$ which are everywhere defined, continuous, and infinitely differentiable. In addition, the functions of $\F$ must be normalizable by an arbitrary multiplicative constant in such a way that the area under the curve $|\psi|^2$ is exactly equal to $1$.

\subsection{Basis of the wave function space}

If $\{\ket{\psi_i}\}$ is a basis for a Hilbert space $\Hbt$ (in particular, the subset $\F$), we can expand every arbitrary vector $\ket{\Psi}$ according to this basis:
\begin{equation}
    \ket{\Psi} = \sum_i c_i \ket{\psi_i}
\end{equation}
so that we have\footnote{$\delta_{ij}$ is known as Kronecker's delta, and is equal to $1$ if $i=j$ and $0$ otherwise.}:
\begin{equation}
    \braket{\psi_i}{\Psi} = \sum_j c_i \braket{\psi_i}{\Psi} = \sum_j c_i\delta_{ij} = c_i
\end{equation}
The choice of basis is arbitrary, and depending on the choice we make, we obtain different representations of state space. There are many different representations, which often have to do with physical properties of the system. 

If you are familiar with basic quantum mechanics, you will probably have seen the position representation of the wave function, $\Psi\left(\vec{r}\right)$. This representation of state space is particularly useful for working with position in a quantum system. However, it is not the \textit{only} representation of state space that can have. What does this mean? Well, here, there is an important concept to understand, which is the difference between a vector and its representation: a vector is a mathematical entity that, once defined, is the same all the time, no matter where we look at it from or which basis we express it in. Its representation, however, may differ, depending on which basis we choose to represent it in. If the basis changes, the coordinates will also change, even though the vector is still the same.

For example, a vector $\vec{v}_B = (a, b)$ expressed in the basis $B=\{(1, 0),\ (0, 1)\}$ will change to $\vec{v}_{B'} = (b/2, a)$ $B'=\{(0, 2),\ (1, 0)\}$. The coordinates of the vector have changed, but we can see that both representations refer to the same vector:

\begin{equation}
    \vec{v}_B = a\cdot \begin{pmatrix}
        1 \\ 0
    \end{pmatrix} + b \cdot \begin{pmatrix}
        0 \\ 1
    \end{pmatrix} = 
    \frac{b}{2}\cdot \begin{pmatrix}
        0 \\ 2
    \end{pmatrix} + a \cdot \begin{pmatrix}
        1 \\ 0
    \end{pmatrix} = \vec{v}_{B'}
\end{equation}

Just as in this example, the position representation of the wave function vector space is only one of its many possible representations. The functions $\psi_i\left(\vec{r}\right)$ form the basis for the position representation of state space. Other representations, like the momentum representation, can be useful in certain situations, as we will see later on. 


\subsection{Representations in state space}

When studying quantum mechanical systems, we need a way to represent quantum states. We do that by choosing an orthonormal basis, either discrete or continuous, in the state space $\F$. Vectors and operators are then represented in this basis by numbers: components for the vectors and matrix elements for the operators.

As we mentioned before, the choice of a representation is, in principle, arbitrary. In fact, it depends on the particular problem being studied: in each case, one chooses the representation that leads to the simplest calculations. 

Most useful bases come as eigenstates of some pertinent operator\footnote{More on this later{\color{red}add link}, but the idea is that, if we represent the state space in the basis of eigenstates (analogous to eigenvectors) of an operator, then that operator will be expressed in that basis as a diagonal matrix, where the elements of the diagonals are the eigenvalues of the operator. This makes calculations very easy.}. So far, we have mentioned the position and the momentum representation. These bases deal with the position and momentum operators, but you can think of many others. For example, the eigenstates of the Hamiltonian for some physical system are often used, especially when solving the Schr√∂dinger equation. These might also be infinite dimensional but can be discrete, as opposed to the continuous bases of position and momentum.

\subsubsection{General representation}

For a general representation of state space (wave function space, $\F$), the elements of $\F$ are functions $\psi(\vec{\xi}),\ \phi(\vec{\xi})$, and the inner product in $\F$ is defined as:

\begin{equation}
    \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) \equiv \int\phi^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
\end{equation}

The length (norm) of a vector is given by:

\begin{equation}
    \text{length}\left(\psi(\vec{\xi})\right)=\sqrt{\left(\psi(\vec{\xi}), \ \psi(\vec{\xi})\right)} = \sqrt{\int\psi^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}} = \sqrt{\int\left|\psi(\vec{\xi}) \right|^2d\vec{\xi}}
\end{equation}

As the vectors of the basis $\{\phi_i\}$ of $\F$ are orthonormal, we have that:
\begin{equation}
    \left(\phi_i(\vec{\xi}), \ \phi_j(\vec{\xi})\right) = \delta_{ij}
\end{equation}

\subsubsection{Discrete orthonormal bases}

A \textbf{discrete orthonormal basis} is defined as:

\begin{definition}
    A countable set of functions $\{u_i\}$ is called orthonormal if:
    \begin{equation}
        \left(u_i(\vec{\xi}), \ u_j(\vec{\xi})\right) = \delta_{ij}
    \end{equation}
    And it constitutes a basis for $\F$ if every function in $\F$ can be written as a linear combination of the functions of the basis in exactly one way:
    \begin{equation}
        \psi(\vec{\xi}) = \sum_i c_i u_i(\vec{\xi})
    \end{equation}
    with the coefficients being:
    \begin{equation}
        c_i = \left(u_i(\vec{\xi}), \ \psi(\vec{\xi})\right) = \int u_i^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
    \end{equation}    
\end{definition}

For a discrete orthonormal basis, we can express the \textbf{scalar product in terms of the components} as:
\begin{equation}
    \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) = \left(\sum_i b_iu_i(\vec{\xi}), \ \sum_j c_ju_j(\vec{\xi})\right) = \sum_{i, \ j} b^*_i c_j \left(u_i(\vec{\xi}), \ u_j(\vec{\xi})\right) = \sum_{i,\ j} b^*_i c_i\delta_{ij} = \sum_i b^*_i c_i
\end{equation}

With a similar proof as in \textbf{Section \ref{closure_relation_proof}}, we can find the closure relation for a discrete orthonormal basis:

\begin{equation*}
    \psi(\vec{\xi}) = \sum_i c_i u_i(\vec{\xi}) = \sum_i \left(u_i(\vec{\xi}), \ \psi(\vec{\xi})\right) u_i(\vec{\xi}) = \sum_i \left( \int u_i^* (\vec{\xi}') \psi(\vec{\xi}')d\vec{\xi}'\right)u_i(\vec{\xi})  =    
\end{equation*}
\begin{equation}
    = \int \left( \sum_i u_i^* (\vec{\xi}') u_i(\vec{\xi})\right)\psi(\vec{\xi}')d\vec{\xi}'
\end{equation}

Therefore, the term in the parenthesis must be equal to $1$ for $\vec{\xi} =\vec{\xi}'$ and zero for every other case, so we obtain the \textbf{closure relation for a discrete orthonormal basis}\footnote{$\delta(\vec{\xi}-\vec{\xi}')$ is known as Dirac's delta function, and it is equal to $1$ if $\vec{\xi} = \vec{\xi}'$ and $0$ otherwise.}:
\begin{equation}
    \sum_i u^*(\vec{\xi}')u_i(\vec{\xi}) = \delta(\vec{\xi} - \vec{\xi}')
\end{equation}

\subsubsection{Continuous orthonormal bases}

A \textbf{continuous orthonormal basis} is defined as:

\begin{definition}
    A continuous set of functions $\{\omega_\alpha\}$, labelled by a continuous index $\alpha$, is called orthonormal if:
    \begin{equation}
        \left(\omega_\alpha(\vec{\xi}), \ \omega_{\alpha'}(\vec{\xi})\right) = \delta(\alpha - \alpha')
    \end{equation}
    And it constitutes a basis for $\F$ if every function in $\F$ can be written as a linear combination of the functions of the basis in exactly one way:
    \begin{equation}
        \psi(\vec{\xi}) = \int c(\alpha) \omega_\alpha(\vec{\xi})d\alpha
    \end{equation}
    with the continuous coefficient being:
    \begin{equation}
        c(\alpha) = \left(\omega_\alpha(\vec{\xi}), \ \psi(\vec{\xi})\right) = \int \omega_\alpha^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
    \end{equation}    
\end{definition}

For a continuous orthonormal basis, we can express the \textbf{scalar product in terms of the continuous coefficients} as:
\begin{equation*}
    \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) = \left(\int b(\alpha) \omega_\alpha(\vec{\xi})d\alpha, \ \int c(\alpha') \omega_{\alpha'}(\vec{\xi})d\alpha'\right) = \sum_{i, \ j} b^*_i c_j \left(u_i(\vec{\xi}), \ u_j(\vec{\xi})\right) = 
\end{equation*}
\begin{equation*}
    = \int\left(\int b^*(\alpha)c(\alpha') (\omega_\alpha(\vec{\xi}), \omega_{\alpha'}(\vec{\xi}))d\alpha\right) d\alpha' = \int\left(\int b^*(\alpha)c(\alpha') \delta(\alpha-\alpha') d\alpha\right) d\alpha'
\end{equation*}
\begin{equation}
    = \int b^*(\alpha)c(\alpha) d\alpha
\end{equation}

With a similar proof as in \textbf{Section \ref{closure_relation_proof}}, we can find the closure relation for a continuous orthonormal basis:

\begin{equation*}
    \psi(\vec{\xi}) = \int c(\alpha) \omega_\alpha(\vec{\xi})d\alpha = \int \left(\omega_\alpha(\vec{\xi}), \ \psi(\vec{\xi})\right) \omega_\alpha(\vec{\xi})d\alpha = \int \left(\int \omega_\alpha^*(\vec{\xi}')\psi(\vec{\xi}')d\vec{\xi}'\right) \omega_\alpha(\vec{\xi})d\alpha  =    
\end{equation*}
\begin{equation}
    = \int \left(\int \omega_\alpha^*(\vec{\xi}')\omega_\alpha(\vec{\xi})d\alpha\right) \psi(\vec{\xi}')d\vec{\xi}'
\end{equation}

Therefore, the term in the parenthesis must be equal to $1$ for $\vec{\xi} =\vec{\xi}'$ and zero for every other case, so we obtain the \textbf{closure relation for a continuous orthonormal basis}:
\begin{equation}
    \int \omega_\alpha^*(\vec{\xi}')\omega_\alpha(\vec{\xi})d\alpha = \left( \omega_\alpha (\vec{\xi}'),\ \omega_\alpha (\vec{\xi}) \right) = \delta(\vec{\xi} - \vec{\xi}')
\end{equation}

\subsubsection{``Bases" not belonging to $\F$}

The bases that we have defined so far, are all in the wave function space $\F$. However, there are other bases that are not in $\F$, but are still useful for representing wave functions. Take the inverse fourier transform of the wave function, for example, which concerns the momentum and the position representations:
\begin{equation}
    \psi(x) = \frac{1}{\sqrt{2\pi\hbar}}\int \overline\psi(p)e^{ip x/\hbar}d p = \int \overline\psi(p)v_p(x)dp, \qquad v_p(x) = \frac{1}{\sqrt{2\pi\hbar}}e^{ip x/\hbar} \text{ (plane wave)}
\end{equation}
Notice that there is a certain parallelism with the continuous orthonormal basis here:
\begin{equation}
    \psi(\vec{\xi}) = \int c(\alpha) \omega_\alpha(\vec{\xi})d\alpha
\end{equation}
where:
\begin{equation}
    \alpha \to p \text{ (continuous index)}, \quad c(\alpha) \to \overline\psi(p), \quad \omega_\alpha(\vec{\xi}) \to v_p(x) \text{ (basis functions)}
\end{equation}
And the ``continuous coefficient'' function $\overline\psi$ can be found as:
\begin{equation}
    \overline\psi(p) = (v_p, \psi) = \int v_p^*(x)\psi(x)dx
\end{equation}


It would seem that the set of ``infinitely continuous'' $v_p$ functions is a basis for $\F$, however, the integral of $\left|v_p(x)\right|^2 = \frac{1}{2\pi\hbar}$ diverges, so $v_p(x)\notin \F$. The usefulness of the continuous bases that we have just introduced is revealed more clearly in what follows. However, we must not lose sight of the following point: a physical state must always correspond to a square-integrable wave function. In no case can $v_p(x)$ represent the state of a particle. These functions are nothing more than intermediaries, very useful in calculations involving operations on the wave functions $\psi(x)$ which are used to describe a physical state.\footnote{\color{red}aclarar}

{\color{red}Add delta function basis}

\subsection{Matrix formulation of quantum mechanics}
\subsubsection{Matrix representation in discrete bases}

Recall that, for a basis $\{\ket{\phi_n}\}$ of $\F$, we can write any $\psi\in\F$ as a linear combination of the basis vectors:

\begin{equation}
    \ket{\psi} = \sum_n c_n\ket{\phi_n}
\end{equation}

where $c_n = \braket{\phi_n|\psi}$ represents the projection of $\ket{\psi}$ onto $\ket{\phi_n}$. So, within the basis $\{\ket{\phi_n}\}$, the ket $\ket{\psi}$ is represented by the set of its components, $c_1, c_2, ...$, along $\ket{\phi_1}, \ket{\phi_2}, ...$, respectively. Hence, we can write the ket $\ket{\psi}$ as a column vector:
\begin{equation}
    \ket{\psi} \to \begin{pmatrix}
        \braket{\phi_1|\psi} \\ \braket{\phi_2|\psi} \\ \vdots \\ \braket{\phi_n|\psi}
    \end{pmatrix} = \begin{pmatrix}
        c_1 \\ c_2 \\ \vdots \\ c_n
    \end{pmatrix}
\end{equation}
The bra $\bra{\psi}$ is represented by the row vector:
\begin{equation}
    \bra{\psi} \to \begin{pmatrix}
        \braket{\psi|\phi_1} & \braket{\psi|\phi_2} & \hdots & \braket{\psi|\phi_n}
    \end{pmatrix} = \begin{pmatrix}
        \braket{\phi_1|\psi}^* & \braket{\phi_2|\psi}^* & \hdots & \braket{\phi_n|\psi}^* 
    \end{pmatrix} =
    \begin{pmatrix}
        c_1^* & c_2^* & \hdots & c_n^*
    \end{pmatrix}
\end{equation}

Just as kets and bras are respresented by column and row vectors, respectively, operators are represented by square matrices.

\subsection{Eigenvalues and eigenvectors of an operator}

We can define the \textbf{eigenvectors of an operator} as:

\begin{definition}
    A state vector $\ket{\psi}$ is said to be an eigenvector (also called eigenket or eigenvector) of an operator $A$ if it is a solution of the eigenvalue equation:
    \begin{equation}
        A\ket{\psi} = a\ket{\psi}
    \end{equation}
    where $a$ is a complex number, called an eigenvalue of $A$.
\end{definition}

Some theorems regarding eigenvectors and eigenvalues are:
\begin{theorem}
    The eigenvalues of the inverse $A^{-1}$ of an operator $A$ are the inverse (with respect to the multiplication, $1/a$) of the eigenvalues $a$ of $A$. \textit{Proof:}
    \begin{equation}
        A\ket{\psi} = a\ket{\psi}\to A^{-1}A\ket{\psi} = aA^{-1}\ket{\psi} \to \ket{\psi} = a A^{-1}\ket{\psi}\to A^{-1}\ket{\psi} = \frac1a \ket{\psi}
    \end{equation}
\end{theorem}
\begin{theorem}
    For a Hermitian operator $A$, all of its eigenvalues are real and the eigenvectors corresponding to different eigenvalues are orthogonal\footnote{Proof in \textbf{Section \ref{eigenvalues_real_orthogonal}}.}.
\end{theorem}
\begin{theorem} \label{commuting_operator_base_thm}
    If two Hermitian operators, $A$ and $B$, commute and if $A$ has no degenerate eigenvalue, then each eigenvector of $A$ is also an eigenvector of $B$. In addition, we can construct a common orthonormal basis that is made of the joint eigenvectors of $A$ and $B$\footnote{Proof in \textbf{Section \ref{commuting_operator_basis}}.}.
\end{theorem}


% As an example, take quantum mechanics on the real line for a particle with a harmonic oscillator potential. You can define states in the position or momentum eigenbasis, but it is most convenient to represent states as an infinite sum over the harmonic oscillator eigenstates. Each eigenstate can itself be written in a position or momentum representation if you want, with the energy eigenstates being given in the position representation by Hermite polynomials.

% Or, take a hyrogen-like atom in three dimensions. The basis of atomic levels and orbitals is much more useful than the position or momentum bases, and the former also comes from the eigenstates of the Hamiltonian.

% Finally, you can make useful quantities out of the position and momentum eigenstates even for Hamiltonians that have no potential energy term. For example, you can use [wave packets](https://en.wikipedia.org/wiki/Wave_packet) as basis states to more naturally talk about how things move through space over time. Position and momentum are great starting points but they are certainly not all that quantum theory has to offer.


% Let's go back to the beginning of the chapter where we talked briefly about basis and representation. So far, we have been talking about Hilbert spaces very generally, and we have talked about performing algebraic operations with vectors in $\Hbt$ without even defining a basis for them. 

% Probably, the most intuitive basis could be a geometrical basis of vectors such as $\vec{e}_x = (1, 0, 0)$, $\vec{e}_y = (0, 1, 0)$ and $\vec{e}_z = (0, 0, 1)$. However, for quantum mechanics, we use \textit{functions} to define our bases. Each of the functions of a basis is a basic state in which the system can be found. Any possible state that the system could be in can be written as a linear combination of the functions of the basis. That is how we build up wave functions, which define the state of a system and belong to the \textbf{wave function space}.

% \begin{definition}\label{wavefunction_space}
%     A wave function space is a Hilbert space $\Hbt$ whose vectors are complex functions $\psi$ (wave functions), which must fulfil the conditions:
    
%     \begin{itemize}
%         \item they must be single valued and continuous.
%         \item they must be infinitely differentiable and square integrable.
%         \item they must be normaliseable by an arbitrary multiplicative constant in such a way that the area under the curve $|\psi|^2$ is exactly equal to $1$.
%         \item they must have a modulus between $0$ and $1$ at any time $t$.
%     \end{itemize}
% \end{definition}

% Although in itself it has no physical interpretation, the wave function of a particle, at a particular time, contains all the information that anybody at that time can have about the particle. For example, applying operators to it, we can obtain information about the particle's energy, position, speed, etc.

% Furthermore, we interpret the square of the modulus of the wave function as a probability density function, giving us the probability of finding the particle at a time $t$ at position $\vec{x}$. The fact that $|\psi|^2$ is a probability density function is the reason for the requirements in \textbf{Definition \ref{wavefunction_space}}. 

% In the wave function space, we can define the \textbf{inner product} as:

% \begin{definition}
%     We define the inner (dot, scalar) product of two vectors $\phi$ and $\psi$ of a wave function space as:
%     \begin{equation}
%         \left(\phi(\vec{\xi}), \ \psi(\vec{\xi})\right) \equiv \int\phi^*(\vec{\xi})\psi(\vec{\xi})d\vec{\xi}
%     \end{equation}
% \end{definition}

% So, these wave functions are the ones composing the basis for our wave function space. How does this connect to what we saw about Dirac? a bra is a vector containing some components that, together with the functions of the basis, define a function within the wave function space. As a simplified example, imagine we have an orthonormal basis $B = \{\phi, \psi\}$. Then, a ket $\ket{f} = (2/\sqrt{5}, 1/\sqrt{5})$ would define the wave function $f(\vec{x}, t) = \frac{2}{\sqrt5}\ \phi + \frac{1}{\sqrt5}\ \psi$.













% \subsection{Physical meaning of the scalar product}

% The scalar product $\braket{\psi}{\phi}$ can be interpreted in two ways:
% \begin{enumerate}
%     \item First, we can interpret it by analogy with vector algebra. The product $\braket{\psi}{\phi}$ represents the projection of $\ket{\phi}$ onto $\bra{\psi}$.
%     \item When working with normalised states $\ket{\psi}$ and $\ket{\phi}$, the inner product $\braket{\psi}{\phi}$ represents the probability amplitude that the system's state $\ket{\psi}$ will, after measurement is performed on the system, be found in another state $\ket{\phi}$.
% \end{enumerate}