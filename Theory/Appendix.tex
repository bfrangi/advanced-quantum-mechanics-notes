\section{Appendix}

\subsection{Linear vector space}

The definition of linear vector space is as follows:

\begin{definition} \label{linear_vector_space}
    A linear vector space consists of two sets of elements and two algebraic rules:
    \begin{itemize}
        \item A set of vectors $\psi, \ \phi, \ \chi, \ ...$ and a set of scalars $a, \ b, \ c, \ ...$.
        \item A rule for adding vectors and a rule for multiplying vectors by scalars.
        \begin{enumerate}
            \item[a)] \textbf{Addition:}
            \begin{itemize}
                \item If $\psi$ and $\phi$ are vectors (elements) of a space, their sum, $\psi + \phi$, is also a vector of the same space.
                \item Commutativity: $\psi + \phi = \phi + \psi$.
                \item Associativity: $(\psi + \phi) + \chi = \psi + (\phi + \chi)$.
                \item Existence of a zero or neutral vector: for each vector $\psi$, there must exist a zero vector $O$ such that: $\psi + O = O + \psi = \psi$.
                \item Existence of a symmetric or inverse vector: each vector $\psi$ must have a symmetric vector $(-\psi)$ such that $\psi + (-\psi) = (-\psi) + \psi = O$.
            \end{itemize}
            \item[b)] \textbf{Multiplication:} The multiplication of vectors by scalars (scalars can be real or complex numbers) has these properties:
            \begin{itemize}
                \item The product of a scalar with a vector gives another vector. In general, if $\psi$ and $\phi$ are two vectors of the space, any linear combination $a\psi + b\phi$ is also a vector of the space, $a$ and $b$ being scalars.
                \item Distributivity with respect to addition: a $(\psi + \phi) = a \psi + a \phi$, and $(a + b)\psi = a\psi + b\psi$.
                \item Associativity with respect to multiplication of scalars: $a (b \psi) = (ab)\psi$
                \item For each element $\psi$ there must exist a unitary scalar $I$ and a zero scalar $0$ such that $I\psi = \psi I = \psi$ and $\psi 0 = 0\psi = O$.
            \end{itemize}
        \end{enumerate}
    \end{itemize}
\end{definition}

\subsection{Linear functionals} \label{linfunct}

In order to understand the mathematical background of the dual space, it is interesting to know the definitions of \textbf{linear maps} and \textbf{linear functionals}. We already defined linear operators in \textbf{Definition \ref{linear_map}}. As for linear functionals:

\begin{definition}
    A linear functional is a linear map $L$ that associates a function with a scalar value, which may be real or complex.
\end{definition}

An example of a linear functional could be the linear map $L_x : \R^2 \to \R$ that returns the $x$-coordinate of the vector it is given. For example:

\begin{equation}
    L_x\begin{bmatrix}
        a \\ b
    \end{bmatrix} = a
\end{equation}

In this case, $L_x$ takes us from $\R^2$ to $\R^1$, so its matrix will be $1$ by $2$ in dimension:

\begin{equation}
    L_x = \begin{bmatrix}
        1 & 0 \\
    \end{bmatrix}
\end{equation}

so that:

\begin{equation}
    L_x\begin{bmatrix}
        a \\ b
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 \\
    \end{bmatrix}
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = a
\end{equation}

Taking a step back, we know that all linear functionals in $\R^2$ take us from $\R^2$ to $\R^1$. Therefore, by definition, all linear functionals in $\R^2$ are represented by $1\times2$ matrices. In other words, the set of all linear functionals in $\R^2$ consists of the set of all row matrices. More generally, the set of all linear functionals in $\R^n$ consists of the set of all $1\times n$ row matrices. In fact, the set of all row matrices, much like column matrices, form their own vector space, which is known as the dual space\footnote{See \textbf{Section \ref{dualspace}} for more on the dual space}.

\subsection{Matrix element of an operator}

The matrix element of an operator $A$ is:
\begin{definition}
    The matrix element of an operator $A$ expressed in the basis $B = \{u_1, u_2, ...\}$ is defined as:
    \begin{equation}
        A_{ij} = \bra{u_i}A\ket{u_j}
    \end{equation}
    where $u_i$ and $u_j$ are the $i$-th and $j$-th vectors of the basis, respectively.
\end{definition}

\subsection{Proof of the Closure relation} \label{closure_relation_proof}

The closure relation states that, for a set of vectors $\{\ket{A_1}, \ket{A_2}, ...\}$ to form a basis of a Hilbert space $\Hbt$, they must fulfil the following relation:

If $B = \{\psi_1, \psi_2, ...\}$ is a basis for a Hilbert space $\Hbt$, we can write any vector $\ket{\psi}\in\Hbt$ as:
\begin{equation}
    \ket{\psi} = \sum_i c_i\ket{A_i}
\end{equation}
Where the coefficients are $c_i = \braket{A_i|\psi}$\footnote{\textit{Proof:} As $A_i$ form an orthonormal basis: $\braket{A_i|\psi} = \bra{A_i}\sum_jc_i\ket{A_j} = \sum_jc_i\braket{A_i|A_j} = \sum_jc_i\delta_{ij} = c_i$}. If we substitute this expression:
\begin{equation}
    \ket{\psi} = \sum_i \braket{A_i|\psi}\ket{A_i}
\end{equation}
As the coefficient is a complex number, we can move it to the end of the expression and separate the inner product:
\begin{equation}
    \ket{\psi} = \sum_i \ket{A_i}\bra{A_i}\ket{\psi}
\end{equation}
As $\ket{\psi}$ is the same for every element of the sum, we can pull it out of the sum:
\begin{equation}
    \ket{\psi} = \left(\sum_i \ket{A_i}\bra{A_i}\right)\ket{\psi}
\end{equation}
And it is now easy to see that this relation will hold if and only if the closure relation holds:
\begin{equation}
    \sum_i \ket{A_i}\bra{A_i} = \Imat
\end{equation}
The inverse argument is easy to follow.

\subsection{Uncertainty relation between two operators} \label{uncertainty_relation}

An interesting application of commutator algebra is to derive a general relation giving the uncertainty product of two Hermitian operators, $A$ and $B$. Let $\braket{A} = \braket{\psi|A|\psi}$ and $\braket{B}=\braket{\psi|B|\psi}$ be the expected values of the operators $A$ and $B$, respectively, with respect to the normalised state vector $\ket{\psi}$. Let $\braket{\hat A} = \braket{A}\Imat$ and $\braket{\hat B} = \braket{B}\Imat$. Introducing the operators $\Delta A$ and $\Delta B$ as:
\begin{equation} \label{eqn:uncertainty_def}
    \Delta A = A - \braket{\hat A}, \qquad \Delta B = B - \braket{\hat B},
\end{equation}
we have $(\Delta A)^2 = A^2 - 2A\braket{\hat A} + \braket{\hat A}^2$. therefore:
\begin{equation}
    \Braket{\psi|(\Delta A)^2|\psi} = \expected{(\Delta A)^2} = \expected{A^2 - 2A\braket{\hat A} + \braket{\hat A}^2} = \expected{A^2} - 2\braket{\hat A}\braket{\hat A} + \braket{\hat A}^2 = \expected{A^2} - \braket{\hat A}^2
\end{equation}

Similarly:
\begin{equation}
    \Braket{\psi|(\Delta B)^2|\psi} = \expected{B^2} - \braket{\hat B}^2
\end{equation}
Then, we can write the uncertainties as:
\begin{equation}
    \Delta A = \sqrt{\expected{(\Delta A)^2}} = \sqrt{\expected{A^2} - \braket{\hat A}^2}, \qquad \Delta B = \sqrt{\expected{(\Delta B)^2}} = \sqrt{\expected{B^2} - \braket{\hat B}^2}
\end{equation}
Writing the action of the operators in \textbf{Equation \ref{eqn:uncertainty_def}} on the state vector $\ket{\psi}$, we have:
\begin{equation}
    \ket{\chi} = \Delta A\ket{\psi} = (A - \braket{\hat A})\ket{\psi}, \qquad \ket{\phi} = \Delta B\ket{\psi} = (B - \braket{\hat B})\ket{\psi}
\end{equation}
The Cauchy-Schwarz inequality\footnote{See for more on this inequality in \href{https://mathworld.wolfram.com/SchwarzsInequality.html}{this} article: \underline{https://shorturl.at/eIKZ9}.} gives us:
\begin{equation}
    \left|\braket{\chi|\phi}\right|^2 \leq \braket{\chi|\chi}\braket{\phi|\phi}
\end{equation}
Since $A$ and $B$ are Hermitian, $\Delta A$ and $\Delta B$ must also be Hermitian, so that: $(\Delta A)^\dagger = \Delta A$ and $(\Delta B)^\dagger = \Delta B$:
\begin{equation}
    \braket{\chi|\chi} = \Braket{\psi|(\Delta A)^\dagger(\Delta A)|\psi} = \Braket{\psi|(\Delta A)^2|\psi} = \expected{(\Delta A)^2}
\end{equation}
\begin{equation}
    \braket{\phi|\phi} = \Braket{\psi|(\Delta B)^\dagger(\Delta B)|\psi} = \Braket{\psi|(\Delta B)^2|\psi} = \expected{(\Delta B)^2}
\end{equation}
\begin{equation}
    \braket{\chi|\phi} = \Braket{\psi|(\Delta A)^\dagger(\Delta B)|\psi} = \Braket{\psi|\Delta A\Delta B|\psi} = \expected{\Delta A\Delta B}
\end{equation}
From this, we obtain a new expression for the Cauchy-Schwarz inequality:
\begin{equation} \label{eqn:cauchy_expected}
    \left|\Braket{\Delta A\Delta B}\right|^2 \leq \expected{(\Delta A)^2}\expected{(\Delta B)^2}
\end{equation}
We can write the $\Delta A \Delta B$ terms of this equation as:
\begin{equation}
    \Delta A \Delta B = 2\cdot \frac12\Delta A \Delta B + \frac12\Delta B \Delta A - \frac12\Delta B \Delta A = \frac12[\Delta A, \Delta B] + \frac12\{\Delta A, \Delta B\}
\end{equation}
Since $[\Delta A, \Delta B] = [A, B]$\footnote{\textit{Proof:} $[\Delta A, \Delta B] = [A, B] + [\braket{A}, \braket{B}] + [\braket{B}, A] + [B, \braket{A}] = [A, B] + \braket{A}\braket{B}\cancel{[\Imat, \Imat]} + \braket{B}\cancel{[\Imat, A]} + \braket{A}\cancel{[B, \Imat]} = [A, B]$.}:
\begin{equation}
    \Delta A \Delta B = \frac12[A, B] + \frac12\{\Delta A, \Delta B\}
\end{equation}

Since $[A, B]$ is anti-Hermitian\footnote{\textit{Proof:} As $A$ and $B$ are Hermitian, then $([A, B])^\dagger = (AB - BA)^\dagger = (AB)^\dagger - (BA)^\dagger = B^\dagger A^\dagger - A^\dagger B^\dagger = BA - AB = -[A, B]$. Then, $[A, B]$ is anti-Hermitian.} and $\{\Delta A, \Delta B\}$ is Hermitian\footnote{\textit{Proof:} As $A$ and $B$ are Hermitian, then $(\{A, B\})^\dagger = (AB + BA)^\dagger = (AB)^\dagger + (BA)^\dagger = B^\dagger A^\dagger + A^\dagger B^\dagger = BA + AB = \{A, B\}$. Then, $\{A, B\}$ is Hermitian.} and since the expectation value of a Hermitian operator is real\footnote{See \textbf{Section     \ref{expected_value_hermitian}} for proof.} and the expectation value of an anti-Hermitian operator is imaginary\footnote{See \textbf{Section     \ref{expected_value_anti_hermitian}} for proof.}, $\expected{\Delta A \Delta B}$ becomes equal to a real part $\frac12\expected{\{\Delta A, \Delta B\}}$ plus an imaginary part $\frac12\expected{[A,B]}$. Then:
\begin{equation}
    \left|\expected{\Delta A \Delta B}\right|^2 = \frac14 \left|\expected{[A, B]}\right|^2 + \frac14 \left|\expected{\{\Delta A , \Delta B\}}\right|^2 \geq \frac14 \left|\expected{[A, B]}\right|^2
\end{equation}

Plugging this into \textbf{Equation \ref{eqn:cauchy_expected}}, we obtain:
\begin{equation}
    \frac14 \left|\expected{[A, B]}\right|^2 \leq \expected{(\Delta A)^2}\expected{(\Delta B)^2}
\end{equation}
Now, taking the square root, we obtain the uncertainty relation:
\begin{equation}
    \Delta A\Delta B \geq \frac12 \left|\expected{[A, B]}\right|
\end{equation}

\begin{definition}
    The uncertainty relation between two Hermitian operators $A$ and $B$ is defined as:
    \begin{equation}
        \Delta A\Delta B \geq \frac12 \left|\expected{[A, B]}\right|
    \end{equation}
\end{definition}

\subsection{Expected value of a Hermitian operator} \label{expected_value_hermitian}

From \textbf{Equation \ref{expected_value}}, we know that the expected value of an operator $A$ in a state $\ket{\phi}$ is $\braket{\phi|A|\phi} = \sum_i|c_i|^2\lambda_i$, where $c_i$ are the components of the vector $\ket{\phi}$ in the basis of eigenvectors $\psi_i$ of $A$, and $\lambda_i$ are the corresponding eigenvalues. If $A$ is Hermitian ($A^\dagger = A$):
\begin{equation}
    \braket{A\psi_i|\psi_i} = \lambda^*||\psi_i||^2
\end{equation}
\begin{equation}
    \braket{\psi_i|A^\dagger\psi_i} = \braket{\psi_i|A\psi_i} = \lambda||\psi_i||^2
\end{equation} 
As $\braket{A\psi_i|\psi_i} = \braket{\psi_i|A^\dagger \psi_i}$, this means that $\lambda^* = \lambda$, so $\lambda$ must be pure real (or zero). Therefore, the expected value of $A$ must also be pure real.

\subsection{Expected value of an anti-Hermitian operator} \label{expected_value_anti_hermitian}

From \textbf{Equation \ref{expected_value}}, we know that the expected value of an operator $A$ in a state $\ket{\phi}$ is $\braket{\phi|A|\phi} = \sum_i|c_i|^2\lambda_i$, where $c_i$ are the components of the vector $\ket{\phi}$ in the basis of eigenvectors $\psi_i$ of $A$, and $\lambda_i$ are the corresponding eigenvalues. If $A$ is anti-Hermitian ($A^\dagger = -A$):
\begin{equation}
    \braket{A\psi_i|\psi_i} = \lambda^*||\psi_i||^2
\end{equation}
\begin{equation}
    \braket{\psi_i|A^\dagger\psi_i} = -\braket{\psi_i|A\psi_i} = -\lambda||\psi_i||^2
\end{equation} 
As $\braket{A\psi_i|\psi_i} = \braket{\psi_i|A^\dagger \psi_i}$, this means that $\lambda^* = -\lambda$, so $\lambda$ must be pure imaginary (or zero). Therefore, the expected value of $A$ must also be pure imaginary.

\subsection{Eigenvalues of a Hermitian operator are real and eigenvectors are orthogonal} \label{eigenvalues_real_orthogonal}

For a Hermitian operator, all of its eigenvalues are real and the eigenvectors corresponding to different eigenvalues are orthogonal. \textit{Proof:}

Note that:
\begin{equation} \label{eigenvalues_real_orthogonal_1}
    A\ket{\psi_n} = a_n\ket{\psi_n} \to \bra{\psi_m}A\ket{\psi_n} = a_n\braket{\psi_m|\psi_n}
\end{equation}
and:
\begin{equation} \label{eigenvalues_real_orthogonal_2}
    \bra{\psi_m} A^{\dagger} = (A\ket{\psi_m})^\dagger = (a_m\ket{\psi_m})^\dagger = a_m^*\bra{\psi_m} \to \bra{\psi_m}A^\dagger\ket{\psi_n} = a_m^*\braket{\psi_m|\psi_n} 
\end{equation}

Subtracting \textbf{Equation \ref{eigenvalues_real_orthogonal_2}} from \textbf{Equation \ref{eigenvalues_real_orthogonal_1}} and using the fact that $A$ is Hermitian ($A^\dagger = A$):
\begin{equation}
    \bra{\psi_m}A\ket{\psi_n} - \bra{\psi_m}A^\dagger\ket{\psi_n} = (a_n - a_m^*)\braket{\psi_m|\psi_n} = 0
\end{equation}

From this:
\begin{itemize}
    \item When $m = n$, we obtain $a_n = a_n^*$, so $a_n$ is real.
    \item Since, in general, $a_n - a_m^* \neq 0$ when $n \neq m$, we have that $\braket{\psi_m|\psi_n} = 0$, so the eigenvectors are orthogonal.
\end{itemize}

\subsection{Common eigenvector basis of two commuting operators} \label{commuting_operator_basis}

This section gives the proof of \textbf{Theorem \ref{commuting_operator_base_thm}}:

Since $A$ has no degenerate eigenvalues\footnote{Degenerate eigenvalues are those that correspond to more than one eigenvector.}, to each eigenvalue of $A$ there corresponds only one eigenvector. We can write the eigenvalue equation as:

\begin{equation}
    A\ket{\phi_n} = a_n\ket{\phi_n}
\end{equation}

And since $A$ commutes with $B$:
\begin{equation}
    AB\ket{\phi_n} = BA\ket{\phi_n} = Ba_n\ket{\phi_n} = a_nB\ket{\phi_n}\to A (B\ket{\phi_n}) = a_n(B\ket{\phi_n})
\end{equation}
This means that $B\ket{\phi_n}$ is an eigenvector of $A$ with eigenvalue $a_n$. But since $A$ has no degenerate eigenvalues, $B\ket{\phi_n}$ must be proportional to $\ket{\phi_n}$, so that $B\ket{\phi_n} = b_n\ket{\phi_n}$. Therefore, $\ket{\phi_n}$ is also an eigenvector of $B$, with eigenvalue $b_n$.

Since each eigenvector of $A$ is also an eigenvector of $B$ (and vice versa), both of these operators must have a common basis. This basis is unique; it is made of the eigenvectors of $A$, which are the same as the eigenvectors of $B$ (we say they are joint eigenvectors of $A$ and $B$). This theorem also holds for any number of mutually commuting Hermitian operators.