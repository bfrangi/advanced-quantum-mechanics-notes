\section{Appendix}

\subsection{Linear vector space}

The definition of linear vector space is as follows:

\begin{definition} \label{linear_vector_space}
    A linear vector space consists of two sets of elements and two algebraic rules:
    \begin{itemize}
        \item A set of vectors $\psi, \ \phi, \ \chi, \ ...$ and a set of scalars $a, \ b, \ c, \ ...$.
        \item A rule for adding vectors and a rule for multiplying vectors by scalars.
        \begin{enumerate}
            \item[a)] \textbf{Addition:}
            \begin{itemize}
                \item If $\psi$ and $\phi$ are vectors (elements) of a space, their sum, $\psi + \phi$, is also a vector of the same space.
                \item Commutativity: $\psi + \phi = \phi + \psi$.
                \item Associativity: $(\psi + \phi) + \chi = \psi + (\phi + \chi)$.
                \item Existence of a zero or neutral vector: for each vector $\psi$, there must exist a zero vector $O$ such that: $\psi + O = O + \psi = \psi$.
                \item Existence of a symmetric or inverse vector: each vector $\psi$ must have a symmetric vector $(-\psi)$ such that $\psi + (-\psi) = (-\psi) + \psi = O$.
            \end{itemize}
            \item[b)] \textbf{Multiplication:} The multiplication of vectors by scalars (scalars can be real or complex numbers) has these properties:
            \begin{itemize}
                \item The product of a scalar with a vector gives another vector. In general, if $\psi$ and $\phi$ are two vectors of the space, any linear combination $a\psi + b\phi$ is also a vector of the space, $a$ and $b$ being scalars.
                \item Distributivity with respect to addition: a $(\psi + \phi) = a \psi + a \phi$, and $(a + b)\psi = a\psi + b\psi$.
                \item Associativity with respect to multiplication of scalars: $a (b \psi) = (ab)\psi$
                \item For each element $\psi$ there must exist a unitary scalar $I$ and a zero scalar ``$0$" such that $I\psi = \psi I = \psi$ and $\psi 0 = 0\psi = O$.
                (2.3)
            \end{itemize}
        \end{enumerate}
    \end{itemize}
\end{definition}

\subsection{Linear functionals} \label{linfunct}

In order to understand the mathematical background of the dual space, it is interesting to know the definitions of \textbf{linear maps} and \textbf{linear functionals}. We already defined linear operators in \textbf{Definition \ref{linear_map}}. As for linear functionals:

\begin{definition}
    A linear functional is a linear map $L$ that associates a function with a scalar value, which may be real or complex.
\end{definition}

An example of a linear functional could be the linear map $L_x : \R^2 \to \R$ that returns the $x$-coordinate of the vector it is given. For example:

\begin{equation}
    L_x\begin{bmatrix}
        a \\ b
    \end{bmatrix} = a
\end{equation}

In this case, $L_x$ takes us from $\R^2$ to $\R^1$, so its matrix will be $1$ by $2$ in dimension:

\begin{equation}
    L_x = \begin{bmatrix}
        1 & 0 \\
    \end{bmatrix}
\end{equation}

so that:

\begin{equation}
    L_x\begin{bmatrix}
        a \\ b
    \end{bmatrix} =
    \begin{bmatrix}
        1 & 0 \\
    \end{bmatrix}\cdot
    \begin{bmatrix}
        a \\ b
    \end{bmatrix} = a
\end{equation}

Taking a step back, we know that all linear functionals in $\R^2$ take us from $\R^2$ to $\R^1$. Therefore, by definition, all linear functionals in $\R^2$ are represented by $1\times2$ matrices. In other words, the set of all linear functionals in $\R^2$ consists of the set of all row matrices. More generally, the set of all linear functionals in $\R^n$ consists of the set of all $1\times n$ row matrices. In fact, the set of all row matrices, much like column matrices, form their own vector space, which is known as the dual space\footnote{See \textbf{Section \ref{dualspace}} for more on the dual space}.

\subsection{Matrix element of an operator}

The matrix element of an operator $A$ is:
\begin{definition}
    The matrix element of an operator $A$ expressed in the basis $B = \{u_1, u_2, ...\}$ is defined as:
    \begin{equation}
        A_{ij} = \bra{u_i}A\ket{u_j}
    \end{equation}
    where $u_i$ and $u_j$ are the $i$-th and $j$-th vectors of the basis, respectively.
\end{definition}

\subsection{Proof of the Closure relation} \label{closure_relation_proof}

The closure relation states that, for a set of vectors $\{\ket{A_1}, \ket{A_2}, ...\}$ to form a basis of a Hilbert space $\Hbt$, they must fulfil the following relation:

If $B = \{\psi_1, \psi_2, ...\}$ is a basis for a Hilbert space $\Hbt$, we can write any vector $\ket{\psi}\in\Hbt$ as:
\begin{equation}
    \ket{\psi} = \sum_i c_i\ket{A_i}
\end{equation}
Where the coefficients are $c_i = \braket{A_i}{\psi}$\footnote{\textit{Proof:} As $A_i$ form an orthonormal basis: $\braket{A_i}{\psi} = \bra{A_i}\sum_jc_i\ket{A_j} = \sum_jc_i\braket{A_i}{A_j} = \sum_jc_i\delta_{ij} = c_i$}. If we substitute this expression:
\begin{equation}
    \ket{\psi} = \sum_i \braket{A_i}{\psi}\ket{A_i}
\end{equation}
As the coefficient is a complex number, we can move it to the end of the expression and separate the inner product:
\begin{equation}
    \ket{\psi} = \sum_i \ket{A_i}\bra{A_i}\ket{\psi}
\end{equation}
As $\ket{\psi}$ is the same for every element of the sum, we can pull it out of the sum:
\begin{equation}
    \ket{\psi} = \left(\sum_i \ket{A_i}\bra{A_i}\right)\ket{\psi}
\end{equation}
And it is now easy to see that this relation will hold if and only if the closure relation holds:
\begin{equation}
    \sum_i \ket{A_i}\bra{A_i} = \Imat
\end{equation}
The inverse argument is easy to follow.

\subsection{Uncertainty relation between two operators} \label{uncertainty_relation}

An interesting application of the commutator algebra is to derive a general relation giving the uncertainties product of two operators, $A$ and $B$. In particular, we want to give a formal derivation of Heisenberg's uncertainty relations.
